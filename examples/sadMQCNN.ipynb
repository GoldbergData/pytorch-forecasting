{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import GroupNormalizer, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.utils import profile\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "warnings.simplefilter(\"error\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQCNNModel(nn.Module):\n",
    "    def __init__(self, static_features, timevarying_features, future_information, time_step, ltsp, lead_future,\n",
    "                 global_hidden_units, horizon_specific_hidden_units,\n",
    "                 horizon_agnostic_hidden_units, local_mlp_hidden_units, local_mlp_output_units):\n",
    "        super(MQCNNModel, self).__init__()\n",
    "        #self.input_tensor = input_tensor\n",
    "        self.time_step = time_step\n",
    "        self.static_features = static_features\n",
    "        self.num_static_features = len(static_features)\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.num_timevarying_features = len(timevarying_features)\n",
    "        self.future_information = future_information\n",
    "        self.num_future_features = len(future_information)\n",
    "        self.ltsp = ltsp\n",
    "        self.lead_future = lead_future\n",
    "        self.global_hidden_units = global_hidden_units\n",
    "        self.horizon_specific_hidden_units = horizon_specific_hidden_units\n",
    "        self.horizon_agnostic_hidden_units = horizon_agnostic_hidden_units\n",
    "        self.local_mlp_hidden_units = local_mlp_hidden_units\n",
    "        self.local_mlp_output_units = local_mlp_output_units\n",
    "\n",
    "        self.encoder = MQCNNEncoder(self.time_step, self.static_features, self.timevarying_features,\n",
    "                                   self.num_static_features, self.num_timevarying_features)\n",
    "        \n",
    "        self.decoder = MQCNNDecoder(self.time_step, self.lead_future, self.ltsp, self.future_information,\n",
    "                                    self.num_future_features, self.global_hidden_units, self.horizon_specific_hidden_units,\n",
    "                                    self.horizon_agnostic_hidden_units, self.local_mlp_hidden_units,\n",
    "                                    self.local_mlp_output_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        output = self.decoder(x, encoding)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MQCNNEncoder(nn.Module):\n",
    "    def __init__(self, time_step, static_features, timevarying_features, num_static_features, num_timevarying_features):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.static_features = static_features\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.num_static_features = num_static_features\n",
    "        self.num_timevarying_features = num_timevarying_features\n",
    "        self.static = StaticLayer(in_channels = self.num_static_features,\n",
    "                                  time_step = self.time_step,\n",
    "                                  static_features = self.static_features)\n",
    "\n",
    "        self.conv = ConvLayer(in_channels = self.num_timevarying_features,\n",
    "                              timevarying_features = self.timevarying_features,\n",
    "                             time_step = self.time_step)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s = self.static(x)\n",
    "        x_t = self.conv(x)\n",
    "        return torch.cat((x_s, x_t), axis = 2)\n",
    "\n",
    "\n",
    "class MQCNNDecoder(nn.Module):\n",
    "    \"\"\"Decoder implementation for MQCNN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config\n",
    "        Configurations\n",
    "    ltsp : list of tuple of int\n",
    "        List of lead-time / span tuples to make predictions for\n",
    "    expander : HybridBlock\n",
    "        Overrides default future data expander if not None\n",
    "    hf1 : HybridBlock\n",
    "        Overrides default global future layer if not None\n",
    "    hf2 : HybridBlock\n",
    "        Overrides default local future layer if not None\n",
    "    ht1 : HybridBlock\n",
    "        Overrides horizon-specific layer if not None\n",
    "    ht2 : HybridBlock\n",
    "        Overrides horizon-agnostic layer if not None\n",
    "    h : HybridBlock\n",
    "        Overrides local MLP if not None\n",
    "    span_1 : HybridBlock\n",
    "        Overrides span 1 layer if not None\n",
    "    span_N : HybridBlock\n",
    "        Overrides span N layer if not None\n",
    "\n",
    "    Inputs:\n",
    "        - **xf** : Future data of shape\n",
    "            (batch_size, Trnn + lead_future - 1, num_future_ts_features)\n",
    "        - **encoded** : Encoded input tensor of shape\n",
    "            (batch_size, Trnn, n) for some n\n",
    "    Outputs:\n",
    "        - **pred_1** :  Span 1 predictions of shape\n",
    "            (batch_size, Trnn, Tpred * num_quantiles)\n",
    "        - **pred_N** : Span N predictions of shape\n",
    "            (batch_size, Trnn, span_N_count * num_quantiles)\n",
    "\n",
    "        In both outputs, the last dimensions has the predictions grouped\n",
    "        together by quantile. For example, the quantiles are P10 and P90\n",
    "        then the span 1 predictions will be:\n",
    "        Tpred_0_p50, Tpred_1_p50, ..., Tpred_N_p50, Tpred_0_p90,\n",
    "        Tpred_1_p90, ... Tpred_N_90\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_step, lead_future, ltsp, future_information, num_future_features,\n",
    "                 global_hidden_units, horizon_specific_hidden_units, horizon_agnostic_hidden_units,\n",
    "                 local_mlp_hidden_units, local_mlp_output_units,\n",
    "                 num_quantiles=2, expander=None, hf1=None, hf2=None,\n",
    "                 ht1=None, ht2=None, h=None, span_1=None, span_N=None,\n",
    "                 **kwargs):\n",
    "        super(MQCNNDecoder, self).__init__(**kwargs)\n",
    "        self.future_features_count = num_future_features\n",
    "        self.future_information = future_information\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.ltsp = ltsp\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.global_hidden_units = global_hidden_units\n",
    "        self.horizon_specific_hidden_units = horizon_specific_hidden_units\n",
    "        self.horizon_agnostic_hidden_units = horizon_agnostic_hidden_units\n",
    "        self.local_mlp_hidden_units = local_mlp_hidden_units\n",
    "        self.local_mlp_output_units = local_mlp_output_units\n",
    "\n",
    "        # We assume that Tpred == span1_count.\n",
    "        # Tpred = forecast_end_index\n",
    "#         self.Tpred = max(map(lambda x: x[0] + x[1], self.ltsp))\n",
    "        self.Tpred = 6\n",
    "#         span1_count = len(list(filter(lambda x: x[1] == 1, self.ltsp)))\n",
    "        span1_count = 1\n",
    "        #print(self.Tpred, span1_count)\n",
    "        #assert span1_count == self.Tpred, f\"Number of span 1 horizons: {span1_count}\\\n",
    "                                            #does not match Tpred: {self.Tpred}\" \n",
    "\n",
    "#         self.spanN_count = len(list(filter(lambda x: x[1] != 1, self.ltsp)))\n",
    "        self.spanN_count = 1\n",
    "        # Setting default components:\n",
    "        if expander is None:\n",
    "            expander = ExpandLayer(self.time_step, self.lead_future, self.future_information)\n",
    "        if hf1 is None:\n",
    "            hf1 = GlobalFutureLayer(self.time_step, self.lead_future, self.future_features_count, out_channels=self.global_hidden_units)\n",
    "        if ht1 is None:\n",
    "            ht1 = HorizonSpecific(self.Tpred, self.time_step, num = self.horizon_specific_hidden_units)\n",
    "        if ht2 is None:\n",
    "            ht2 = HorizonAgnostic(self.horizon_agnostic_hidden_units, self.lead_future)\n",
    "        if h is None:\n",
    "            h = LocalMlp(self.local_mlp_hidden_units, self.local_mlp_output_units)\n",
    "        if span_1 is None:\n",
    "            span_1 = Span1(self.time_step, self.lead_future, self.num_quantiles)\n",
    "        if span_N is None:\n",
    "            span_N = SpanN(self.time_step, self.lead_future, self.num_quantiles, self.spanN_count)\n",
    "\n",
    "        self.expander = expander\n",
    "        self.hf1 = hf1\n",
    "        self.hf2 = hf2\n",
    "        self.ht1 = ht1\n",
    "        self.ht2 = ht2\n",
    "        self.h = h\n",
    "        self.span_1 = span_1\n",
    "        self.span_N = span_N\n",
    "\n",
    "    def forward(self, x, encoded):\n",
    "        xf = x['future_information']\n",
    "        expanded = self.expander(xf)\n",
    "        hf1 = self.hf1(expanded)\n",
    "        hf2 = F.relu(expanded)\n",
    "        \n",
    "        ht = torch.cat((encoded, hf1), dim=-1)\n",
    "        ht1 = self.ht1(ht)\n",
    "        ht2 = self.ht2(ht)\n",
    "        h = torch.cat((ht1, ht2, hf2), dim=-1)\n",
    "        h = self.h(h)\n",
    "        return self.span_1(h)\n",
    "\n",
    "# submodule\n",
    "\n",
    "class StaticLayer(nn.Module):\n",
    "    def __init__(self, in_channels, time_step, static_features, out_channels = 30, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        #self.static_features = static_features\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.static = nn.Linear(self.in_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x['static_features'][:,:1,:]\n",
    "        x = self.dropout(x)\n",
    "        x = self.static(x)\n",
    "        return x.repeat(1, self.time_step, 1)\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, time_step, timevarying_features, in_channels, out_channels = 30, kernel_size = 2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.time_step = time_step\n",
    "\n",
    "        self.c1 = nn.Conv1d(self.in_channels, self.out_channels, self.kernel_size, dilation = 1)\n",
    "        self.c2 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 2)\n",
    "        self.c3 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size,  dilation = 4)\n",
    "        self.c4 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 8)\n",
    "        self.c5 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 16)\n",
    "        self.c6 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_t = x['timevarying_features'][:, :self.time_step, :]\n",
    "        x_t = x_t.permute(0, 2, 1)\n",
    "        x_t = F.pad(x_t, (1,0), \"constant\", 0)\n",
    "        x_t = self.c1(x_t)\n",
    "        x_t = F.pad(x_t, (2,0), \"constant\", 0)\n",
    "        x_t = self.c2(x_t)\n",
    "        x_t = F.pad(x_t, (4,0), \"constant\", 0)\n",
    "        x_t = self.c3(x_t)\n",
    "        x_t = F.pad(x_t, (8,0), \"constant\", 0)\n",
    "        x_t = self.c4(x_t)\n",
    "        x_t = F.pad(x_t, (16,0), \"constant\", 0)\n",
    "        x_t = self.c5(x_t)\n",
    "        \n",
    "        return x_t.permute(0, 2, 1)\n",
    "\n",
    "class ExpandLayer(nn.Module):\n",
    "    \"\"\"Expands the dimension referred to as `expand_axis` into two\n",
    "    dimensions by applying a sliding window. For example, a tensor of\n",
    "    shape (1, 4, 2) as follows:\n",
    "\n",
    "    [[[0. 1.]\n",
    "      [2. 3.]\n",
    "      [4. 5.]\n",
    "      [6. 7.]]]\n",
    "\n",
    "    where `expand_axis` = 1 and `time_step` = 3 (number of windows) and\n",
    "    `lead_future` = 2 (window length) will become:\n",
    "\n",
    "    [[[[0. 1.]\n",
    "       [2. 3.]]\n",
    "\n",
    "      [[2. 3.]\n",
    "       [4. 5.]]\n",
    "\n",
    "      [[4. 5.]\n",
    "       [6. 7.]]]]\n",
    "\n",
    "    Used for expanding future information tensors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_step : int\n",
    "        Length of the time sequence (number of windows)\n",
    "    lead_future : int\n",
    "        Number of future time points (window length)\n",
    "    expand_axis : int\n",
    "        Axis to expand\"\"\"\n",
    "\n",
    "    def __init__(self, time_step, lead_future, future_information, **kwargs):\n",
    "        super(ExpandLayer, self).__init__(**kwargs)\n",
    "    \n",
    "        self.time_step = time_step\n",
    "        self.future_information = future_information\n",
    "        self.lead_future = lead_future\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First create a matrix of indices, which we will use to slice\n",
    "        # `input` along `expand_axis`. For example, for time_step=3 and\n",
    "        # lead_future=2,\n",
    "        # idx = [[0. 1.]\n",
    "        #        [1. 2.]\n",
    "        #        [2. 3.]]\n",
    "        # We achieve this by doing a broadcast add of\n",
    "        # [[0.] [1.] [2.]] and [[0. 1.]]\n",
    "        idx = torch.add(torch.arange(self.time_step).unsqueeze(axis = 1),\n",
    "                        torch.arange(self.lead_future).unsqueeze(axis = 0))\n",
    "        # Now we slice `input`, taking elements from `input` that correspond to\n",
    "        # the indices in `idx` along the `expand_axis` dimension\n",
    "        return x[:, idx, :]\n",
    "\n",
    "        \n",
    "class GlobalFutureLayer(nn.Module):\n",
    "    def __init__(self, time_step, lead_future, future_features_count, out_channels = 30):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.future_features_count = future_features_count\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.l1 = nn.Linear(self.lead_future * self.future_features_count, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(-1, self.time_step, self.lead_future * self.future_features_count)\n",
    "        \n",
    "        return self.l1(x)\n",
    "    \n",
    "class HorizonSpecific(nn.Module):\n",
    "    def __init__(self, Tpred, time_step, num = 20):\n",
    "        super().__init__()\n",
    "        self.Tpred = Tpred\n",
    "        self.time_step = time_step\n",
    "        self.num = num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.Tpred * self.num)(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x.view(-1, self.time_step, self.Tpred, 20)\n",
    "\n",
    "class HorizonAgnostic(nn.Module):\n",
    "    def __init__(self, out_channels, lead_future):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.lead_future = lead_future\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.out_channels)(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.unsqueeze(axis = 2)\n",
    "        x = x.repeat(1,1, self.lead_future, 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class LocalMlp(nn.Module):\n",
    "    def __init__(self, hidden, output):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.hidden)(x)\n",
    "        x = F.relu(x)\n",
    "        x = nn.Linear(self.hidden, self.output)(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Span1(nn.Module):\n",
    "    def __init__(self, time_step, lead_future, num_quantiles):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.num_quantiles = num_quantiles\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.num_quantiles)(x)\n",
    "        x = F.relu(x.contiguous().view(-1, x.size(-2), x.size(-1)))\n",
    "        x = x.view(-1, self.time_step, self.lead_future, self.num_quantiles)\n",
    "        x = x.view(-1, self.time_step, self.lead_future*self.num_quantiles)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpanN(nn.Module):\n",
    "    def __init__(self, time_step, lead_future, num_quantiles, spanN_count):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.spanN_count = spanN_count\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.contiguous().view(-1, self.time_step, x.size(-2) * x.size(-1))\n",
    "\n",
    "        x = nn.Linear(x.size(-1), self.spanN_count * self.num_quantiles)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_stallion_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, static_features, timevarying_features, future_information, \n",
    "                 target, train_time_step, predict_time_step, num_quantiles):\n",
    "        \n",
    "        self.data = data\n",
    "        self.train_time_step = train_time_step\n",
    "        self.predict_time_step = predict_time_step\n",
    "        self.num_quantiles = num_quantiles\n",
    "        \n",
    "        \n",
    "        self.static_features = torch.tensor(self.data.\\\n",
    "                              loc[self.data['time_idx'] < self.train_time_step][static_features].\\\n",
    "                to_numpy(np.float64).reshape(-1, self.train_time_step, len(static_features))).float()\n",
    "        \n",
    "        self.timevarying_features = torch.tensor(self.data.\\\n",
    "                              loc[self.data['time_idx'] < self.train_time_step][timevarying_features].\\\n",
    "                to_numpy(np.float64).reshape(-1, self.train_time_step, len(timevarying_features))).float()\n",
    "            \n",
    "        self.future_information = torch.tensor(self.data[future_information].\\\n",
    "                to_numpy(np.float64).reshape(-1, (self.train_time_step + self.predict_time_step), len(future_information))).float()\n",
    "        \n",
    "        self.targets = torch.tensor(self.data.\\\n",
    "                              loc[self.data['time_idx'] >= self.train_time_step][target].\\\n",
    "                            to_numpy(np.float64).reshape(-1, self.predict_time_step, len(target))).float()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.timevarying_features.shape[1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        static_features = self.static_features[idx, :, :]\n",
    "        timevarying_features = self.timevarying_features[idx, :, :]\n",
    "        future_information = self.future_information[idx, :, :]\n",
    "        targets = self.targets[idx, :, :].\\\n",
    "                        repeat_interleave(self.num_quantiles, dim = -1)\n",
    "        \n",
    "        return dict(static_features = static_features, timevarying_features = timevarying_features,\n",
    "                    future_information = future_information, targets = targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>sku</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>...</th>\n",
       "      <th>independence_day</th>\n",
       "      <th>revolution_day_memorial</th>\n",
       "      <th>regional_games</th>\n",
       "      <th>fifa_u_17_world_cup</th>\n",
       "      <th>football_gold_cup</th>\n",
       "      <th>beer_capital</th>\n",
       "      <th>music_fest</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Agency_25</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>25.845238</td>\n",
       "      <td>1264.162234</td>\n",
       "      <td>1152.473405</td>\n",
       "      <td>111.688829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.835008</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Agency_29</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>8.7480</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>498567142</td>\n",
       "      <td>762225057</td>\n",
       "      <td>27.584615</td>\n",
       "      <td>1316.098485</td>\n",
       "      <td>1296.804924</td>\n",
       "      <td>19.293561</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.465966</td>\n",
       "      <td>177</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19532</th>\n",
       "      <td>Agency_47</td>\n",
       "      <td>SKU_01</td>\n",
       "      <td>4.9680</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>454252482</td>\n",
       "      <td>789624076</td>\n",
       "      <td>30.665957</td>\n",
       "      <td>1269.250000</td>\n",
       "      <td>1266.490490</td>\n",
       "      <td>2.759510</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217413</td>\n",
       "      <td>322</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Agency_53</td>\n",
       "      <td>SKU_07</td>\n",
       "      <td>21.6825</td>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>480693900</td>\n",
       "      <td>791658684</td>\n",
       "      <td>29.197727</td>\n",
       "      <td>1193.842373</td>\n",
       "      <td>1128.124395</td>\n",
       "      <td>65.717978</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.504745</td>\n",
       "      <td>240</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>Agency_17</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>960.5520</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>515468092</td>\n",
       "      <td>871204688</td>\n",
       "      <td>23.608120</td>\n",
       "      <td>1338.334248</td>\n",
       "      <td>1232.128069</td>\n",
       "      <td>106.206179</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.935699</td>\n",
       "      <td>259</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>Agency_05</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>1184.6535</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>425528909</td>\n",
       "      <td>734443953</td>\n",
       "      <td>28.668254</td>\n",
       "      <td>1369.556376</td>\n",
       "      <td>1161.135214</td>\n",
       "      <td>208.421162</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.218151</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19204</th>\n",
       "      <td>Agency_11</td>\n",
       "      <td>SKU_05</td>\n",
       "      <td>5.5593</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>623319783</td>\n",
       "      <td>1049868815</td>\n",
       "      <td>31.915385</td>\n",
       "      <td>1922.486644</td>\n",
       "      <td>1651.307674</td>\n",
       "      <td>271.178970</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.105636</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>Agency_48</td>\n",
       "      <td>SKU_04</td>\n",
       "      <td>4275.1605</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>509281531</td>\n",
       "      <td>892192092</td>\n",
       "      <td>26.767857</td>\n",
       "      <td>1761.258209</td>\n",
       "      <td>1546.059670</td>\n",
       "      <td>215.198539</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.218455</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>Agency_07</td>\n",
       "      <td>SKU_21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>544203593</td>\n",
       "      <td>761469815</td>\n",
       "      <td>28.987755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>Agency_21</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>46.3608</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>589969396</td>\n",
       "      <td>940912941</td>\n",
       "      <td>32.478910</td>\n",
       "      <td>1675.922116</td>\n",
       "      <td>1413.571789</td>\n",
       "      <td>262.350327</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.654088</td>\n",
       "      <td>181</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          agency     sku     volume       date  industry_volume  soda_volume  \\\n",
       "291    Agency_25  SKU_03     0.5076 2013-01-01        492612703    718394219   \n",
       "871    Agency_29  SKU_02     8.7480 2015-01-01        498567142    762225057   \n",
       "19532  Agency_47  SKU_01     4.9680 2013-09-01        454252482    789624076   \n",
       "2089   Agency_53  SKU_07    21.6825 2013-10-01        480693900    791658684   \n",
       "9755   Agency_17  SKU_02   960.5520 2015-03-01        515468092    871204688   \n",
       "7561   Agency_05  SKU_03  1184.6535 2014-02-01        425528909    734443953   \n",
       "19204  Agency_11  SKU_05     5.5593 2017-08-01        623319783   1049868815   \n",
       "8781   Agency_48  SKU_04  4275.1605 2013-03-01        509281531    892192092   \n",
       "2540   Agency_07  SKU_21     0.0000 2015-10-01        544203593    761469815   \n",
       "12084  Agency_21  SKU_03    46.3608 2017-04-01        589969396    940912941   \n",
       "\n",
       "       avg_max_temp  price_regular  price_actual    discount  ...  \\\n",
       "291       25.845238    1264.162234   1152.473405  111.688829  ...   \n",
       "871       27.584615    1316.098485   1296.804924   19.293561  ...   \n",
       "19532     30.665957    1269.250000   1266.490490    2.759510  ...   \n",
       "2089      29.197727    1193.842373   1128.124395   65.717978  ...   \n",
       "9755      23.608120    1338.334248   1232.128069  106.206179  ...   \n",
       "7561      28.668254    1369.556376   1161.135214  208.421162  ...   \n",
       "19204     31.915385    1922.486644   1651.307674  271.178970  ...   \n",
       "8781      26.767857    1761.258209   1546.059670  215.198539  ...   \n",
       "2540      28.987755       0.000000      0.000000    0.000000  ...   \n",
       "12084     32.478910    1675.922116   1413.571789  262.350327  ...   \n",
       "\n",
       "       independence_day  revolution_day_memorial  regional_games  \\\n",
       "291                   0                        0               0   \n",
       "871                   0                        0               0   \n",
       "19532                 1                        0               0   \n",
       "2089                  0                        0               0   \n",
       "9755                  0                        0               0   \n",
       "7561                  0                        0               0   \n",
       "19204                 0                        0               0   \n",
       "8781                  0                        0               0   \n",
       "2540                  0                        0               0   \n",
       "12084                 0                        0               0   \n",
       "\n",
       "       fifa_u_17_world_cup  football_gold_cup  beer_capital  music_fest  \\\n",
       "291                      0                  0             0           0   \n",
       "871                      0                  0             0           0   \n",
       "19532                    0                  0             0           0   \n",
       "2089                     0                  0             1           0   \n",
       "9755                     0                  0             0           1   \n",
       "7561                     0                  0             0           0   \n",
       "19204                    0                  0             0           0   \n",
       "8781                     0                  0             0           1   \n",
       "2540                     0                  0             0           0   \n",
       "12084                    0                  0             0           0   \n",
       "\n",
       "       discount_in_percent  timeseries  time_idx  \n",
       "291               8.835008         228         0  \n",
       "871               1.465966         177        24  \n",
       "19532             0.217413         322         8  \n",
       "2089              5.504745         240         9  \n",
       "9755              7.935699         259        26  \n",
       "7561             15.218151          21        13  \n",
       "19204            14.105636          17        55  \n",
       "8781             12.218455         151         2  \n",
       "2540              0.000000         300        33  \n",
       "12084            15.654088         181        51  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# add additional features\n",
    "\n",
    "# show sample data\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = data.sort_values(['agency', 'sku', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = pd.get_dummies(data_sorted, columns=['month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols=['avg_population_2017']\n",
    "timevarying_cols=['volume', 'industry_volume', 'soda_volume', 'price_regular']\n",
    "future_cols=['month_1', 'month_2','month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "                    'month_9', 'month_10', 'month_11', 'month_12', 'price_regular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = Dataset(data_sorted,\n",
    "                   static_features = static_cols,\n",
    "                   timevarying_features = timevarying_cols,\n",
    "                   future_information = future_cols,\n",
    "                   target=['volume'], \n",
    "                   train_time_step=54, \n",
    "                   predict_time_step=6,\n",
    "                   num_quantiles = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(training, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MQCNNModel(static_cols, timevarying_cols, future_cols, \n",
    "                   54, [(0, 1), (1, 1)], 6, 50, 20, 100, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MQCNNModel(\n",
       "  (encoder): MQCNNEncoder(\n",
       "    (static): StaticLayer(\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "      (static): Linear(in_features=1, out_features=30, bias=True)\n",
       "    )\n",
       "    (conv): ConvLayer(\n",
       "      (c1): Conv1d(4, 30, kernel_size=(2,), stride=(1,))\n",
       "      (c2): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(2,))\n",
       "      (c3): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(4,))\n",
       "      (c4): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(8,))\n",
       "      (c5): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(16,))\n",
       "      (c6): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(32,))\n",
       "    )\n",
       "  )\n",
       "  (decoder): MQCNNDecoder(\n",
       "    (expander): ExpandLayer()\n",
       "    (hf1): GlobalFutureLayer(\n",
       "      (l1): Linear(in_features=78, out_features=50, bias=True)\n",
       "    )\n",
       "    (ht1): HorizonSpecific()\n",
       "    (ht2): HorizonAgnostic()\n",
       "    (h): LocalMlp()\n",
       "    (span_1): Span1()\n",
       "    (span_N): SpanN()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "           33593.8828,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 197224.4531,\n",
      "           29842.8926, 344622.7188],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 209273.6094,\n",
      "               0.0000, 287197.6562],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 188868.8281,\n",
      "               0.0000, 299368.2500]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "           33593.4609,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 197224.1406,\n",
      "           29842.6426, 344622.2812],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 209273.5938,\n",
      "               0.0000, 287197.5625],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 188868.9219,\n",
      "               0.0000, 299368.4062]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "           33191.4219,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 197090.0312,\n",
      "           32883.9219, 346597.6250],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 208443.0156,\n",
      "               0.0000, 288964.0938],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 188038.3125,\n",
      "               0.0000, 301135.8125]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "           33196.2656,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 197089.1875,\n",
      "           32878.9844, 346594.7500],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 208442.3594,\n",
      "               0.0000, 288960.0938],\n",
      "         [     0.0000,      0.0000,      0.0000,  ..., 188037.3438,\n",
      "               0.0000, 301132.0312]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[0.0000e+00, 3.3252e+04, 4.7645e+03,  ..., 2.2131e+04,\n",
      "          4.5054e+04, 2.9939e+04],\n",
      "         [0.0000e+00, 1.1504e+05, 0.0000e+00,  ..., 8.9234e+04,\n",
      "          0.0000e+00, 4.0155e+04],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 4.5177e+05, 0.0000e+00,  ..., 2.3044e+05,\n",
      "          0.0000e+00, 3.7302e+05],\n",
      "         [0.0000e+00, 4.6909e+05, 1.9069e+04,  ..., 2.7990e+05,\n",
      "          0.0000e+00, 3.8542e+05],\n",
      "         [0.0000e+00, 4.9374e+05, 0.0000e+00,  ..., 2.9815e+05,\n",
      "          0.0000e+00, 4.2965e+05]],\n",
      "\n",
      "        [[0.0000e+00, 3.3252e+04, 4.7679e+03,  ..., 2.2127e+04,\n",
      "          4.5059e+04, 2.9935e+04],\n",
      "         [0.0000e+00, 1.1504e+05, 0.0000e+00,  ..., 8.9233e+04,\n",
      "          0.0000e+00, 4.0153e+04],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 4.5177e+05, 0.0000e+00,  ..., 2.3044e+05,\n",
      "          0.0000e+00, 3.7301e+05],\n",
      "         [0.0000e+00, 4.6909e+05, 1.9072e+04,  ..., 2.7990e+05,\n",
      "          0.0000e+00, 3.8542e+05],\n",
      "         [0.0000e+00, 4.9374e+05, 0.0000e+00,  ..., 2.9815e+05,\n",
      "          0.0000e+00, 4.2965e+05]],\n",
      "\n",
      "        [[3.0996e+03, 3.2980e+04, 4.9242e+03,  ..., 2.0185e+04,\n",
      "          4.7056e+04, 2.7211e+04],\n",
      "         [0.0000e+00, 1.1671e+05, 0.0000e+00,  ..., 8.4994e+04,\n",
      "          0.0000e+00, 3.9882e+04],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 4.5197e+05, 0.0000e+00,  ..., 2.3082e+05,\n",
      "          0.0000e+00, 3.7046e+05],\n",
      "         [0.0000e+00, 4.6910e+05, 2.0016e+04,  ..., 2.8039e+05,\n",
      "          0.0000e+00, 3.8268e+05],\n",
      "         [0.0000e+00, 4.9427e+05, 3.4784e+02,  ..., 2.9851e+05,\n",
      "          0.0000e+00, 4.2727e+05]],\n",
      "\n",
      "        [[3.0990e+03, 3.2980e+04, 4.9238e+03,  ..., 2.0185e+04,\n",
      "          4.7055e+04, 2.7212e+04],\n",
      "         [0.0000e+00, 1.1671e+05, 0.0000e+00,  ..., 8.4994e+04,\n",
      "          0.0000e+00, 3.9882e+04],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 4.5197e+05, 0.0000e+00,  ..., 2.3082e+05,\n",
      "          0.0000e+00, 3.7047e+05],\n",
      "         [0.0000e+00, 4.6910e+05, 2.0016e+04,  ..., 2.8039e+05,\n",
      "          0.0000e+00, 3.8268e+05],\n",
      "         [0.0000e+00, 4.9427e+05, 3.4810e+02,  ..., 2.9851e+05,\n",
      "          0.0000e+00, 4.2727e+05]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [ 51347.4531,      0.0000,  46859.6719,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [142237.1250,      0.0000,  53650.2539,  ...,      0.0000,\n",
      "           65976.0312,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [ 51354.6953,      0.0000,  46861.7578,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [142241.0781,      0.0000,  53650.4336,  ...,      0.0000,\n",
      "           65982.2188,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000]],\n",
      "\n",
      "        [[ 13042.6982,      0.0000,  20430.7090,  ...,      0.0000,\n",
      "            3501.0691,      0.0000],\n",
      "         [118055.2500,      0.0000, 146878.6094,  ...,      0.0000,\n",
      "           22326.6113,      0.0000],\n",
      "         [195762.0469,      0.0000,  98125.7188,  ...,      0.0000,\n",
      "          122702.2891,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [ 51325.7148,      0.0000,  46851.7344,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [142223.7500,      0.0000,  53647.0312,  ...,      0.0000,\n",
      "           65958.1484,      0.0000],\n",
      "         ...,\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[ 88014.9375,   7070.7593,  58166.9297,  ...,      0.0000,\n",
      "           90371.8359,      0.0000],\n",
      "         [118526.7969,      0.0000,  19860.0918,  ...,  11567.6816,\n",
      "           97071.1719,      0.0000],\n",
      "         [306924.3438,      0.0000, 311207.0938,  ...,      0.0000,\n",
      "          396623.0312,      0.0000],\n",
      "         ...,\n",
      "         [720076.5625, 311458.3750, 755899.8125,  ..., 329067.7812,\n",
      "          728997.6250, 179981.8906],\n",
      "         [781803.5625, 336154.4688, 853441.1875,  ..., 354304.3125,\n",
      "          826537.3125, 207186.6094],\n",
      "         [792296.7500, 348076.8125, 817475.7500,  ..., 360742.5000,\n",
      "          813775.5625, 209461.7656]],\n",
      "\n",
      "        [[100157.4375,   6993.5483,  82872.2812,  ...,  12773.3564,\n",
      "           90500.4141,      0.0000],\n",
      "         [153591.5469,      0.0000,  82666.6719,  ...,   3119.7615,\n",
      "           91700.7969,      0.0000],\n",
      "         [257673.2188,   4540.0562, 281752.0312,  ...,      0.0000,\n",
      "          373100.2812,      0.0000],\n",
      "         ...,\n",
      "         [752883.4375, 294249.1875, 798517.2500,  ..., 312190.8750,\n",
      "          722656.2500, 179514.0156],\n",
      "         [812145.1875, 320652.8438, 884674.3125,  ..., 340272.1562,\n",
      "          825968.5625, 208338.9688],\n",
      "         [825094.1875, 330696.0000, 849437.6250,  ..., 345931.0625,\n",
      "          814481.6875, 208812.2656]],\n",
      "\n",
      "        [[100157.4531,   6993.5601,  82872.2969,  ...,  12773.3662,\n",
      "           90500.4219,      0.0000],\n",
      "         [153591.5469,      0.0000,  82666.6172,  ...,   3119.7751,\n",
      "           91700.7500,      0.0000],\n",
      "         [257673.2344,   4540.1030, 281752.0000,  ...,      0.0000,\n",
      "          373100.2812,      0.0000],\n",
      "         ...,\n",
      "         [752883.3750, 294249.5625, 798526.8125,  ..., 312190.0938,\n",
      "          722656.6250, 179514.2656],\n",
      "         [812149.3125, 320656.7812, 884676.0000,  ..., 340271.9688,\n",
      "          825970.5625, 208340.4531],\n",
      "         [825093.0625, 330695.8125, 849437.1250,  ..., 345930.5625,\n",
      "          814483.5000, 208816.1406]],\n",
      "\n",
      "        [[100158.5312,   6994.4497,  82870.8516,  ...,  12773.9863,\n",
      "           90498.8594,      0.0000],\n",
      "         [153591.5156,      0.0000,  82667.0938,  ...,   3121.3982,\n",
      "           91690.8125,      0.0000],\n",
      "         [257670.5156,   4541.1304, 281748.5312,  ...,      0.0000,\n",
      "          373096.9688,      0.0000],\n",
      "         ...,\n",
      "         [752883.3125, 294249.1250, 798526.5625,  ..., 312189.3125,\n",
      "          722657.4375, 179514.3594],\n",
      "         [812149.1250, 320657.0000, 884676.1875,  ..., 340271.8125,\n",
      "          825971.3750, 208341.2344],\n",
      "         [825093.1250, 330695.8750, 849436.7500,  ..., 345930.3438,\n",
      "          814483.4375, 208816.9531]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[    0.0000, 16484.9824, 10888.4814,  ..., 31856.6113,\n",
      "              0.0000,  3137.7747],\n",
      "         [ 1941.3658, 45406.2812,     0.0000,  ..., 68205.8281,\n",
      "              0.0000,     0.0000],\n",
      "         [  107.7564,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         ...,\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,  1601.1957,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,   194.3412,     0.0000,  ..., 24386.6387,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         ...,\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000, 16471.8047, 10881.2861,  ..., 31856.9238,\n",
      "              0.0000,  3132.2849],\n",
      "         [ 1937.2838, 45403.9609,     0.0000,  ..., 68204.6328,\n",
      "              0.0000,     0.0000],\n",
      "         [   96.1197,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         ...,\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,  1601.5590,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,   194.4974,     0.0000,  ..., 24387.6348,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         ...,\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[  55679.0312,       0.0000,   99826.6484,  ...,       0.0000,\n",
      "            88189.8906,       0.0000],\n",
      "         [  34191.9414,       0.0000,  115537.6719,  ...,       0.0000,\n",
      "           108998.0312,       0.0000],\n",
      "         [ 213916.8906,   36747.4727,  266750.8438,  ...,  143918.2812,\n",
      "           162491.8906,   83071.8359],\n",
      "         ...,\n",
      "         [ 828910.8750,       0.0000, 1431682.6250,  ...,       0.0000,\n",
      "          1262353.2500,       0.0000],\n",
      "         [1015400.0000,       0.0000, 1508635.3750,  ...,       0.0000,\n",
      "          1377555.2500,       0.0000],\n",
      "         [1027047.1875,       0.0000, 1547715.5000,  ...,       0.0000,\n",
      "          1403818.8750,       0.0000]],\n",
      "\n",
      "        [[  44996.2109,       0.0000,   92752.2969,  ...,       0.0000,\n",
      "            68004.8359,       0.0000],\n",
      "         [  41300.3633,       0.0000,  116036.5156,  ...,       0.0000,\n",
      "           123863.6641,       0.0000],\n",
      "         [ 167089.9531,   24275.7832,  244236.2188,  ...,  134830.9062,\n",
      "           142436.6719,   64828.5625],\n",
      "         ...,\n",
      "         [ 838975.8750,       0.0000, 1410705.0000,  ...,       0.0000,\n",
      "          1264723.6250,       0.0000],\n",
      "         [1016412.2500,       0.0000, 1477012.2500,  ...,       0.0000,\n",
      "          1369462.8750,       0.0000],\n",
      "         [1024179.0000,       0.0000, 1521750.6250,  ...,       0.0000,\n",
      "          1396639.1250,       0.0000]],\n",
      "\n",
      "        [[  44644.7266,       0.0000,   92908.6641,  ...,       0.0000,\n",
      "            68293.0078,       0.0000],\n",
      "         [  41304.9023,       0.0000,  116046.2891,  ...,       0.0000,\n",
      "           124453.6484,       0.0000],\n",
      "         [ 166083.1094,   23441.5781,  243597.9219,  ...,  133859.5312,\n",
      "           141435.0781,   64071.6953],\n",
      "         ...,\n",
      "         [ 839289.0000,       0.0000, 1409854.6250,  ...,       0.0000,\n",
      "          1264742.0000,       0.0000],\n",
      "         [1016690.0000,       0.0000, 1476067.7500,  ...,       0.0000,\n",
      "          1369538.1250,       0.0000],\n",
      "         [1024227.6250,       0.0000, 1520743.2500,  ...,       0.0000,\n",
      "          1396466.3750,       0.0000]],\n",
      "\n",
      "        [[  44997.4570,       0.0000,   92753.4531,  ...,       0.0000,\n",
      "            68005.7344,       0.0000],\n",
      "         [  41301.1523,       0.0000,  116036.8672,  ...,       0.0000,\n",
      "           123864.0312,       0.0000],\n",
      "         [ 167091.4219,   24276.0234,  244237.1719,  ...,  134831.2969,\n",
      "           142437.6406,   64828.7969],\n",
      "         ...,\n",
      "         [ 838976.8750,       0.0000, 1410704.7500,  ...,       0.0000,\n",
      "          1264724.2500,       0.0000],\n",
      "         [1016412.3125,       0.0000, 1477011.8750,  ...,       0.0000,\n",
      "          1369463.1250,       0.0000],\n",
      "         [1024178.8125,       0.0000, 1521750.5000,  ...,       0.0000,\n",
      "          1396639.3750,       0.0000]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[ 22593.6758,      0.0000,  24805.0488,  ...,      0.0000,\n",
      "           21263.2227,      0.0000],\n",
      "         [ 42376.7148,      0.0000,   2509.3765,  ...,      0.0000,\n",
      "           69690.6250,      0.0000],\n",
      "         [ 57352.6094,      0.0000,  60335.6250,  ...,      0.0000,\n",
      "           19739.6797,      0.0000],\n",
      "         ...,\n",
      "         [358200.0000,      0.0000, 568881.3125,  ..., 233684.3281,\n",
      "          465680.9062, 238106.8750],\n",
      "         [381706.3125,      0.0000, 539851.7500,  ..., 194026.1562,\n",
      "          423898.5312, 208373.0938],\n",
      "         [381220.5625,      0.0000, 563549.5000,  ..., 200430.9844,\n",
      "          435630.5000, 212103.1562]],\n",
      "\n",
      "        [[ 23253.6152,      0.0000,  25811.8789,  ...,      0.0000,\n",
      "           21770.6641,      0.0000],\n",
      "         [ 42798.1836,      0.0000,   3317.3433,  ...,      0.0000,\n",
      "           70267.3828,      0.0000],\n",
      "         [ 57574.2812,      0.0000,  60492.7031,  ...,      0.0000,\n",
      "           20046.2988,      0.0000],\n",
      "         ...,\n",
      "         [358267.1562,      0.0000, 568868.1250,  ..., 232937.8438,\n",
      "          466430.7500, 238014.2812],\n",
      "         [381642.0938,      0.0000, 539861.7500,  ..., 193271.1562,\n",
      "          423982.6250, 208209.0312],\n",
      "         [381216.0000,      0.0000, 563567.1875,  ..., 199707.7969,\n",
      "          436419.3125, 211884.9688]],\n",
      "\n",
      "        [[ 26256.3750,      0.0000,   7316.2104,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [ 36286.5898,      0.0000,      0.0000,  ...,      0.0000,\n",
      "           79898.6953,      0.0000],\n",
      "         [ 42507.1172,      0.0000,  52003.7773,  ...,      0.0000,\n",
      "           12406.4785,      0.0000],\n",
      "         ...,\n",
      "         [355487.7500,  39964.6406, 568630.8750,  ..., 281370.5312,\n",
      "          415248.3125, 240898.3125],\n",
      "         [384955.9375,  43196.5234, 541586.3750,  ..., 244106.1562,\n",
      "          396237.5625, 216397.0156],\n",
      "         [376060.5000,  21806.5566, 563491.5000,  ..., 249172.1562,\n",
      "          396129.0000, 218229.1719]],\n",
      "\n",
      "        [[ 26257.1543,      0.0000,   7316.8159,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [ 36286.3867,      0.0000,      0.0000,  ...,      0.0000,\n",
      "           79898.8047,      0.0000],\n",
      "         [ 42508.1055,      0.0000,  52004.8828,  ...,      0.0000,\n",
      "           12406.9785,      0.0000],\n",
      "         ...,\n",
      "         [355493.8438,  39963.6094, 568633.1875,  ..., 281371.8125,\n",
      "          415255.1250, 240896.3281],\n",
      "         [384964.7500,  43196.5352, 541588.1250,  ..., 244109.7344,\n",
      "          396241.6250, 216393.4688],\n",
      "         [376064.2812,  21805.0723, 563493.3125,  ..., 249176.0312,\n",
      "          396132.3125, 218226.8594]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[ 87579.6875,      0.0000,  48640.8906,  ...,      0.0000,\n",
      "           44204.1328,      0.0000],\n",
      "         [207034.3906,      0.0000, 262556.0938,  ...,      0.0000,\n",
      "          197142.0312,      0.0000],\n",
      "         [222114.9062,      0.0000, 163335.9844,  ...,      0.0000,\n",
      "           96206.3203,      0.0000],\n",
      "         ...,\n",
      "         [401608.6250,      0.0000,      0.0000,  ..., 382730.1250,\n",
      "               0.0000, 779025.3125],\n",
      "         [545912.1250,      0.0000,      0.0000,  ..., 314721.7188,\n",
      "           57166.9609, 677396.9375],\n",
      "         [607255.5625,      0.0000,      0.0000,  ..., 306472.8438,\n",
      "           46826.5859, 702397.0000]],\n",
      "\n",
      "        [[ 87580.1250,      0.0000,  48639.8594,  ...,      0.0000,\n",
      "           44203.6562,      0.0000],\n",
      "         [207031.3906,      0.0000, 262555.1562,  ...,      0.0000,\n",
      "          197139.9062,      0.0000],\n",
      "         [222120.1875,      0.0000, 163341.3125,  ...,      0.0000,\n",
      "           96211.7656,      0.0000],\n",
      "         ...,\n",
      "         [401609.7188,      0.0000,      0.0000,  ..., 382722.3125,\n",
      "               0.0000, 779016.8750],\n",
      "         [545909.3125,      0.0000,      0.0000,  ..., 314714.7188,\n",
      "           57172.5234, 677382.1250],\n",
      "         [607256.1875,      0.0000,      0.0000,  ..., 306460.0938,\n",
      "           46832.3672, 702382.1875]],\n",
      "\n",
      "        [[ 87579.7422,      0.0000,  48640.8086,  ...,      0.0000,\n",
      "           44204.3672,      0.0000],\n",
      "         [207033.8906,      0.0000, 262555.9688,  ...,      0.0000,\n",
      "          197141.7031,      0.0000],\n",
      "         [222115.2031,      0.0000, 163336.0156,  ...,      0.0000,\n",
      "           96206.3984,      0.0000],\n",
      "         ...,\n",
      "         [401609.6875,      0.0000,      0.0000,  ..., 382728.5312,\n",
      "               0.0000, 779021.1875],\n",
      "         [545913.1250,      0.0000,      0.0000,  ..., 314719.2812,\n",
      "           57169.8359, 677391.1250],\n",
      "         [607255.5625,      0.0000,      0.0000,  ..., 306471.9688,\n",
      "           46828.5547, 702391.5625]],\n",
      "\n",
      "        [[ 99275.8516,      0.0000, 113158.2656,  ...,      0.0000,\n",
      "           63791.1094,      0.0000],\n",
      "         [258414.7500,      0.0000, 285402.6250,  ...,      0.0000,\n",
      "          237846.5625,      0.0000],\n",
      "         [264159.6562,      0.0000, 209644.2812,  ...,      0.0000,\n",
      "          104666.0234,      0.0000],\n",
      "         ...,\n",
      "         [391028.3750,      0.0000,      0.0000,  ..., 441795.6875,\n",
      "               0.0000, 855000.1250],\n",
      "         [513966.4688,      0.0000,      0.0000,  ..., 357758.3438,\n",
      "           20330.3379, 718868.0000],\n",
      "         [600329.5000,      0.0000,      0.0000,  ..., 327199.2188,\n",
      "           13410.5557, 735492.9375]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[8.8669e+04, 0.0000e+00, 1.2111e+04,  ..., 0.0000e+00,\n",
      "          4.1970e+04, 0.0000e+00],\n",
      "         [1.8249e+05, 1.2421e+04, 1.6614e+05,  ..., 0.0000e+00,\n",
      "          2.4919e+05, 0.0000e+00],\n",
      "         [1.3195e+04, 0.0000e+00, 1.0986e+04,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.6540e+04],\n",
      "         ...,\n",
      "         [2.4504e+05, 5.9582e+05, 1.6761e+05,  ..., 3.3941e+05,\n",
      "          4.0995e+05, 1.1609e+05],\n",
      "         [3.0821e+05, 5.5984e+05, 2.6674e+05,  ..., 2.7482e+05,\n",
      "          5.3727e+05, 8.7646e+04],\n",
      "         [2.8051e+05, 5.8748e+05, 2.7522e+05,  ..., 3.0228e+05,\n",
      "          5.1936e+05, 9.1173e+04]],\n",
      "\n",
      "        [[8.8669e+04, 0.0000e+00, 1.2111e+04,  ..., 0.0000e+00,\n",
      "          4.1970e+04, 0.0000e+00],\n",
      "         [1.8249e+05, 1.2421e+04, 1.6614e+05,  ..., 0.0000e+00,\n",
      "          2.4919e+05, 0.0000e+00],\n",
      "         [1.3195e+04, 0.0000e+00, 1.0986e+04,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 1.6540e+04],\n",
      "         ...,\n",
      "         [2.4506e+05, 5.9579e+05, 1.6760e+05,  ..., 3.3939e+05,\n",
      "          4.0993e+05, 1.1608e+05],\n",
      "         [3.0822e+05, 5.5983e+05, 2.6675e+05,  ..., 2.7479e+05,\n",
      "          5.3727e+05, 8.7647e+04],\n",
      "         [2.8052e+05, 5.8746e+05, 2.7519e+05,  ..., 3.0226e+05,\n",
      "          5.1937e+05, 9.1157e+04]],\n",
      "\n",
      "        [[1.2845e+05, 0.0000e+00, 7.0561e+04,  ..., 0.0000e+00,\n",
      "          1.2148e+05, 0.0000e+00],\n",
      "         [1.7763e+05, 0.0000e+00, 2.2295e+05,  ..., 0.0000e+00,\n",
      "          3.0052e+05, 0.0000e+00],\n",
      "         [7.5464e+03, 0.0000e+00, 2.3061e+04,  ..., 0.0000e+00,\n",
      "          2.1224e+04, 0.0000e+00],\n",
      "         ...,\n",
      "         [3.3357e+05, 5.0363e+05, 2.0973e+05,  ..., 3.3303e+05,\n",
      "          4.2379e+05, 1.3098e+05],\n",
      "         [3.1274e+05, 5.2107e+05, 2.8485e+05,  ..., 2.8372e+05,\n",
      "          5.4918e+05, 1.0680e+05],\n",
      "         [3.4525e+05, 5.0905e+05, 2.9423e+05,  ..., 3.2357e+05,\n",
      "          4.9995e+05, 1.3213e+05]],\n",
      "\n",
      "        [[1.0008e+05, 0.0000e+00, 4.5335e+04,  ..., 0.0000e+00,\n",
      "          7.2555e+04, 0.0000e+00],\n",
      "         [1.9362e+05, 1.6775e+02, 1.9956e+05,  ..., 0.0000e+00,\n",
      "          2.8842e+05, 0.0000e+00],\n",
      "         [8.9486e+03, 0.0000e+00, 1.4035e+04,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 8.6756e+03],\n",
      "         ...,\n",
      "         [2.7796e+05, 5.5926e+05, 1.8200e+05,  ..., 3.3693e+05,\n",
      "          4.2377e+05, 1.2075e+05],\n",
      "         [2.9576e+05, 5.5400e+05, 2.7039e+05,  ..., 2.7432e+05,\n",
      "          5.4770e+05, 9.3462e+04],\n",
      "         [3.0138e+05, 5.5758e+05, 2.8139e+05,  ..., 3.0908e+05,\n",
      "          5.1687e+05, 1.0390e+05]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[     0.0000,      0.0000,      0.0000,  ...,   5482.0527,\n",
      "               0.0000,  28640.7969],\n",
      "         [     0.0000,  33716.4453,      0.0000,  ...,  74481.0547,\n",
      "               0.0000, 121287.9688],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,   9586.5410,\n",
      "               0.0000,  71430.3750],\n",
      "         ...,\n",
      "         [     0.0000,  91964.4297,      0.0000,  ..., 306752.0000,\n",
      "               0.0000, 332655.0000],\n",
      "         [     0.0000,  43665.1484,      0.0000,  ..., 301340.0000,\n",
      "               0.0000, 295813.5938],\n",
      "         [     0.0000,  79647.0234,      0.0000,  ..., 321531.4062,\n",
      "               0.0000, 350420.1562]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,   5484.0449,\n",
      "               0.0000,  28642.6055],\n",
      "         [     0.0000,  33717.1016,      0.0000,  ...,  74482.9766,\n",
      "               0.0000, 121289.1484],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,   9586.8086,\n",
      "               0.0000,  71429.2656],\n",
      "         ...,\n",
      "         [     0.0000,  91964.6328,      0.0000,  ..., 306752.2500,\n",
      "               0.0000, 332655.2500],\n",
      "         [     0.0000,  43665.0391,      0.0000,  ..., 301340.5938,\n",
      "               0.0000, 295814.1250],\n",
      "         [     0.0000,  79647.3203,      0.0000,  ..., 321531.8438,\n",
      "               0.0000, 350420.7188]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,  19805.2676],\n",
      "         [     0.0000,  32674.4043,      0.0000,  ...,  79981.2422,\n",
      "               0.0000, 104503.8203],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,   6360.8999,\n",
      "               0.0000,  85937.8984],\n",
      "         ...,\n",
      "         [     0.0000,  64546.2031,      0.0000,  ..., 321471.8438,\n",
      "               0.0000, 328889.2188],\n",
      "         [     0.0000,  50407.5781,      0.0000,  ..., 326253.6875,\n",
      "               0.0000, 299389.8750],\n",
      "         [     0.0000,  67262.2422,      0.0000,  ..., 335592.8750,\n",
      "               0.0000, 350272.0312]],\n",
      "\n",
      "        [[     0.0000,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,  19801.8457],\n",
      "         [     0.0000,  32668.6152,      0.0000,  ...,  79975.8594,\n",
      "               0.0000, 104497.0000],\n",
      "         [     0.0000,      0.0000,      0.0000,  ...,   6358.2183,\n",
      "               0.0000,  85935.1484],\n",
      "         ...,\n",
      "         [     0.0000,  64543.7578,      0.0000,  ..., 321470.7812,\n",
      "               0.0000, 328890.3750],\n",
      "         [     0.0000,  50405.1953,      0.0000,  ..., 326254.5625,\n",
      "               0.0000, 299391.6875],\n",
      "         [     0.0000,  67260.1562,      0.0000,  ..., 335591.3125,\n",
      "               0.0000, 350272.5312]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[ 22120.8027,      0.0000,  27768.9121,  ...,      0.0000,\n",
      "           24000.3438,   3276.4131],\n",
      "         [ 34257.4141,      0.0000,  26838.7480,  ...,      0.0000,\n",
      "           61596.4805,      0.0000],\n",
      "         [ 98973.9609, 163964.1562, 123757.3281,  ..., 290136.2500,\n",
      "          107945.9766, 201241.1094],\n",
      "         ...,\n",
      "         [459137.8125, 416907.5000, 598200.3125,  ..., 371688.8750,\n",
      "          456397.0000,  42200.9961],\n",
      "         [493219.6250, 478605.0625, 585212.0000,  ..., 317433.2812,\n",
      "          414998.2188,      0.0000],\n",
      "         [515000.1562, 524248.8438, 566677.8125,  ..., 329374.1875,\n",
      "          440379.5625,  15374.4951]],\n",
      "\n",
      "        [[ 22818.5117,      0.0000,  28155.7734,  ...,      0.0000,\n",
      "           24022.4199,   3274.4966],\n",
      "         [ 34494.8281,      0.0000,  26868.0918,  ...,      0.0000,\n",
      "           61624.9844,      0.0000],\n",
      "         [ 98549.0000, 165417.3438, 122070.7344,  ..., 289861.0625,\n",
      "          107649.4766, 202130.9531],\n",
      "         ...,\n",
      "         [459027.5312, 415580.0625, 597636.3750,  ..., 370973.3438,\n",
      "          456300.7500,  42779.7773],\n",
      "         [493716.8438, 477720.5938, 584565.5625,  ..., 316616.3438,\n",
      "          415114.3125,      0.0000],\n",
      "         [515443.1250, 523353.5000, 565871.7500,  ..., 328611.7500,\n",
      "          440157.6562,  15171.6924]],\n",
      "\n",
      "        [[ 22821.5898,      0.0000,  28149.0254,  ...,      0.0000,\n",
      "           24019.2012,   3274.6421],\n",
      "         [ 34497.6328,      0.0000,  26857.5547,  ...,      0.0000,\n",
      "           61617.2656,      0.0000],\n",
      "         [ 98551.8984, 165415.9531, 122069.5547,  ..., 289857.2188,\n",
      "          107653.5078, 202130.0312],\n",
      "         ...,\n",
      "         [459027.7812, 415579.9688, 597636.6250,  ..., 370973.1875,\n",
      "          456301.0000,  42779.8516],\n",
      "         [493717.0625, 477720.7812, 584565.7500,  ..., 316616.7188,\n",
      "          415114.4375,      0.0000],\n",
      "         [515443.1875, 523353.4375, 565871.8750,  ..., 328611.4375,\n",
      "          440157.9375,  15171.3975]],\n",
      "\n",
      "        [[ 22117.4570,      0.0000,  27776.3242,  ...,      0.0000,\n",
      "           24003.9023,   3276.2559],\n",
      "         [ 34254.3750,      0.0000,  26852.4512,  ...,      0.0000,\n",
      "           61604.9609,      0.0000],\n",
      "         [ 98970.8984, 163965.9688, 123758.5859,  ..., 290140.6250,\n",
      "          107941.5703, 201242.4062],\n",
      "         ...,\n",
      "         [459137.5312, 416910.0938, 598194.6875,  ..., 371692.2812,\n",
      "          456397.4375,  42204.4023],\n",
      "         [493214.2812, 478602.0000, 585225.0625,  ..., 317430.4688,\n",
      "          414998.0312,      0.0000],\n",
      "         [515015.3750, 524246.8750, 566681.3125,  ..., 329371.5625,\n",
      "          440388.1562,  15380.5625]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[      0.0000,   58444.9102,   43913.6172,  ...,   52716.4883,\n",
      "                0.0000,   26944.9258],\n",
      "         [      0.0000,       0.0000,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         [      0.0000,   29816.9395,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         ...,\n",
      "         [      0.0000,  900833.5000,   34870.3086,  ...,  908688.2500,\n",
      "                0.0000,  848521.9375],\n",
      "         [      0.0000,  817615.7500,  212180.6250,  ...,  958717.3750,\n",
      "                0.0000,  873317.2500],\n",
      "         [      0.0000,  841329.6250,  222302.7812,  ...,  944218.3750,\n",
      "                0.0000,  890778.3125]],\n",
      "\n",
      "        [[      0.0000,   58462.0742,   43936.8633,  ...,   52747.6602,\n",
      "                0.0000,   26943.1270],\n",
      "         [      0.0000,       0.0000,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         [      0.0000,   29803.1484,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         ...,\n",
      "         [      0.0000,  900832.8125,   34874.4336,  ...,  908690.0625,\n",
      "                0.0000,  848520.7500],\n",
      "         [      0.0000,  817612.0000,  212182.6875,  ...,  958718.0625,\n",
      "                0.0000,  873314.8125],\n",
      "         [      0.0000,  841326.3125,  222305.1562,  ...,  944219.7500,\n",
      "                0.0000,  890775.2500]],\n",
      "\n",
      "        [[      0.0000,   45936.3984,    1235.5608,  ...,   48307.7383,\n",
      "                0.0000,   21290.5840],\n",
      "         [      0.0000,       0.0000,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         [      0.0000,   41772.8516,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         ...,\n",
      "         [      0.0000,  920672.8125,   87839.9297,  ...,  968055.0000,\n",
      "                0.0000,  928973.1875],\n",
      "         [      0.0000,  828225.5000,  248153.7812,  ..., 1004814.6250,\n",
      "                0.0000,  950389.6875],\n",
      "         [      0.0000,  861459.4375,  264233.6250,  ...,  997040.6250,\n",
      "                0.0000,  969342.1250]],\n",
      "\n",
      "        [[      0.0000,   45936.6406,    1236.2522,  ...,   48307.5508,\n",
      "                0.0000,   21290.4023],\n",
      "         [      0.0000,       0.0000,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         [      0.0000,   41772.7227,       0.0000,  ...,       0.0000,\n",
      "                0.0000,       0.0000],\n",
      "         ...,\n",
      "         [      0.0000,  920672.3750,   87838.5547,  ...,  968056.2500,\n",
      "                0.0000,  928970.7500],\n",
      "         [      0.0000,  828225.6250,  248152.0312,  ..., 1004815.7500,\n",
      "                0.0000,  950387.6250],\n",
      "         [      0.0000,  861459.8125,  264230.1562,  ...,  997040.1250,\n",
      "                0.0000,  969339.5000]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[  53299.8555,    7181.6826,   45168.3750,  ...,       0.0000,\n",
      "            80674.7266,   37170.5742],\n",
      "         [ 190110.9219,   41827.5156,  189240.9219,  ...,   11758.6191,\n",
      "           210313.4219,   74910.8125],\n",
      "         [  78441.6406,       0.0000,       0.0000,  ...,       0.0000,\n",
      "            14417.9844,       0.0000],\n",
      "         ...,\n",
      "         [1078610.3750,  415196.0312, 1004396.7500,  ...,  419391.3438,\n",
      "           983436.4375,  324679.2812],\n",
      "         [1096069.0000,  369336.4688, 1042753.9375,  ...,  456642.6562,\n",
      "           991698.2500,  313780.8125],\n",
      "         [1114740.0000,  383443.6562, 1028174.2500,  ...,  418782.5312,\n",
      "           982488.2500,  316830.2500]],\n",
      "\n",
      "        [[  53300.3359,    7180.6704,   45171.0234,  ...,       0.0000,\n",
      "            80678.5938,   37173.7031],\n",
      "         [ 190112.0469,   41829.2500,  189243.2500,  ...,   11763.4180,\n",
      "           210319.4531,   74916.7891],\n",
      "         [  78439.7578,       0.0000,       0.0000,  ...,       0.0000,\n",
      "            14421.8906,       0.0000],\n",
      "         ...,\n",
      "         [1078620.0000,  415205.9688, 1004406.6875,  ...,  419400.1250,\n",
      "           983445.5000,  324688.8438],\n",
      "         [1096080.3750,  369345.4062, 1042765.5000,  ...,  456646.1250,\n",
      "           991709.1875,  313791.2188],\n",
      "         [1114749.8750,  383452.9062, 1028184.1875,  ...,  418786.1875,\n",
      "           982499.2500,  316840.8750]],\n",
      "\n",
      "        [[  34570.8164,       0.0000,   24487.4863,  ...,       0.0000,\n",
      "            72822.8203,   23005.6660],\n",
      "         [ 175334.9844,   12663.7002,  175350.7656,  ...,       0.0000,\n",
      "           213357.2344,   72086.7812],\n",
      "         [  52849.3945,       0.0000,       0.0000,  ...,       0.0000,\n",
      "            17988.3984,       0.0000],\n",
      "         ...,\n",
      "         [1074509.1250,  385893.5312,  992912.6250,  ...,  412798.7188,\n",
      "           964923.9375,  310919.6562],\n",
      "         [1092352.1250,  353492.0625, 1027914.8750,  ...,  438461.5625,\n",
      "           973141.4375,  301948.6562],\n",
      "         [1114256.5000,  364902.2812, 1017713.5625,  ...,  401435.7500,\n",
      "           967008.0625,  305157.1250]],\n",
      "\n",
      "        [[  53298.7773,    7184.4521,   45161.1953,  ...,       0.0000,\n",
      "            80663.5938,   37161.7148],\n",
      "         [ 190109.5625,   41824.4141,  189234.9531,  ...,   11745.6641,\n",
      "           210300.0312,   74896.6328],\n",
      "         [  78447.1172,       0.0000,       0.0000,  ...,       0.0000,\n",
      "            14408.8711,       0.0000],\n",
      "         ...,\n",
      "         [1078619.5000,  415208.5938, 1004406.8750,  ...,  419401.7500,\n",
      "           983448.3750,  324690.7500],\n",
      "         [1096081.3750,  369346.2812, 1042768.0625,  ...,  456647.0312,\n",
      "           991709.8750,  313791.5312],\n",
      "         [1114752.5000,  383454.8438, 1028186.7500,  ...,  418785.8750,\n",
      "           982499.6250,  316841.3750]]], grad_fn=<ViewBackward>)\n",
      "tensor([[[ 17757.0137,      0.0000,      0.0000,  ...,  11888.0703,\n",
      "               0.0000,      0.0000],\n",
      "         [   818.2797,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,  20998.1113,  ...,      0.0000,\n",
      "           24454.2246,      0.0000],\n",
      "         ...,\n",
      "         [182248.6875,      0.0000, 199595.4531,  ...,      0.0000,\n",
      "          214976.0312,      0.0000],\n",
      "         [271298.4062,      0.0000, 253216.7812,  ...,      0.0000,\n",
      "          260749.4219,      0.0000],\n",
      "         [271307.8125,      0.0000, 266109.7500,  ...,      0.0000,\n",
      "          287027.4688,      0.0000]],\n",
      "\n",
      "        [[ 18215.2480,      0.0000,      0.0000,  ...,  13541.4902,\n",
      "               0.0000,      0.0000],\n",
      "         [  1677.5219,      0.0000,      0.0000,  ...,      0.0000,\n",
      "               0.0000,      0.0000],\n",
      "         [     0.0000,      0.0000,  19721.6855,  ...,      0.0000,\n",
      "           23400.9121,      0.0000],\n",
      "         ...,\n",
      "         [181742.8281,      0.0000, 198868.7656,  ...,      0.0000,\n",
      "          214593.7188,      0.0000],\n",
      "         [270716.6562,      0.0000, 252577.8906,  ...,      0.0000,\n",
      "          260299.9844,      0.0000],\n",
      "         [270770.4062,      0.0000, 265225.5625,  ...,      0.0000,\n",
      "          286564.8750,      0.0000]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "     model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MQCNNModel(\n",
       "  (encoder): MQCNNEncoder(\n",
       "    (static): StaticLayer(\n",
       "      (dropout): Dropout(p=0.4, inplace=False)\n",
       "      (static): Linear(in_features=1, out_features=30, bias=True)\n",
       "    )\n",
       "    (conv): ConvLayer(\n",
       "      (c1): Conv1d(4, 30, kernel_size=(2,), stride=(1,))\n",
       "      (c2): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(2,))\n",
       "      (c3): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(4,))\n",
       "      (c4): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(8,))\n",
       "      (c5): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(16,))\n",
       "      (c6): Conv1d(30, 30, kernel_size=(2,), stride=(1,), dilation=(32,))\n",
       "    )\n",
       "  )\n",
       "  (decoder): MQCNNDecoder(\n",
       "    (expander): ExpandLayer()\n",
       "    (hf1): GlobalFutureLayer(\n",
       "      (l1): Linear(in_features=78, out_features=50, bias=True)\n",
       "    )\n",
       "    (ht1): HorizonSpecific()\n",
       "    (ht2): HorizonAgnostic()\n",
       "    (h): LocalMlp()\n",
       "    (span_1): Span1()\n",
       "    (span_N): SpanN()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
