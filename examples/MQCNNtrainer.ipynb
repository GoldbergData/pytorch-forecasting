{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as torch\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import GroupNormalizer, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting.utils import profile\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "warnings.simplefilter(\"error\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQCNNEncoder(nn.Module):\n",
    "    def __init__(self, time_step, static_features, timevarying_features, num_static_features, num_timevarying_features):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.static_features = static_features\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.num_static_features = num_static_features\n",
    "        self.num_timevarying_features = num_timevarying_features\n",
    "        self.static = StaticLayer(in_channels = self.num_static_features,\n",
    "                                  time_step = self.time_step,\n",
    "                                  static_features = self.static_features)\n",
    "\n",
    "        self.conv = ConvLayer(in_channels = self.num_timevarying_features,\n",
    "                              timevarying_features = self.timevarying_features,\n",
    "                             time_step = self.time_step)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s = self.static(x)\n",
    "        x_t = self.conv(x)\n",
    "        return torch.cat((x_s, x_t), axis = 2)\n",
    "\n",
    "\n",
    "class MQCNNDecoder(nn.Module):\n",
    "    \"\"\"Decoder implementation for MQCNN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config\n",
    "        Configurations\n",
    "    ltsp : list of tuple of int\n",
    "        List of lead-time / span tuples to make predictions for\n",
    "    expander : HybridBlock\n",
    "        Overrides default future data expander if not None\n",
    "    hf1 : HybridBlock\n",
    "        Overrides default global future layer if not None\n",
    "    hf2 : HybridBlock\n",
    "        Overrides default local future layer if not None\n",
    "    ht1 : HybridBlock\n",
    "        Overrides horizon-specific layer if not None\n",
    "    ht2 : HybridBlock\n",
    "        Overrides horizon-agnostic layer if not None\n",
    "    h : HybridBlock\n",
    "        Overrides local MLP if not None\n",
    "    span_1 : HybridBlock\n",
    "        Overrides span 1 layer if not None\n",
    "    span_N : HybridBlock\n",
    "        Overrides span N layer if not None\n",
    "\n",
    "    Inputs:\n",
    "        - **xf** : Future data of shape\n",
    "            (batch_size, Trnn + lead_future - 1, num_future_ts_features)\n",
    "        - **encoded** : Encoded input tensor of shape\n",
    "            (batch_size, Trnn, n) for some n\n",
    "    Outputs:\n",
    "        - **pred_1** :  Span 1 predictions of shape\n",
    "            (batch_size, Trnn, Tpred * num_quantiles)\n",
    "        - **pred_N** : Span N predictions of shape\n",
    "            (batch_size, Trnn, span_N_count * num_quantiles)\n",
    "\n",
    "        In both outputs, the last dimensions has the predictions grouped\n",
    "        together by quantile. For example, the quantiles are P10 and P90\n",
    "        then the span 1 predictions will be:\n",
    "        Tpred_0_p50, Tpred_1_p50, ..., Tpred_N_p50, Tpred_0_p90,\n",
    "        Tpred_1_p90, ... Tpred_N_90\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_step, lead_future, ltsp, future_information, num_future_features,\n",
    "                 global_hidden_units, horizon_specific_hidden_units, horizon_agnostic_hidden_units,\n",
    "                 local_mlp_hidden_units, local_mlp_output_units,\n",
    "                 num_quantiles=2, expander=None, hf1=None, hf2=None,\n",
    "                 ht1=None, ht2=None, h=None, span_1=None, span_N=None,\n",
    "                 **kwargs):\n",
    "        super(MQCNNDecoder, self).__init__(**kwargs)\n",
    "        self.future_features_count = num_future_features\n",
    "        self.future_information = future_information\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.ltsp = ltsp\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.global_hidden_units = global_hidden_units\n",
    "        self.horizon_specific_hidden_units = horizon_specific_hidden_units\n",
    "        self.horizon_agnostic_hidden_units = horizon_agnostic_hidden_units\n",
    "        self.local_mlp_hidden_units = local_mlp_hidden_units\n",
    "        self.local_mlp_output_units = local_mlp_output_units\n",
    "\n",
    "        # We assume that Tpred == span1_count.\n",
    "        # Tpred = forecast_end_index\n",
    "#         self.Tpred = max(map(lambda x: x[0] + x[1], self.ltsp))\n",
    "        self.Tpred = 6\n",
    "#         span1_count = len(list(filter(lambda x: x[1] == 1, self.ltsp)))\n",
    "        span1_count = 1\n",
    "        #print(self.Tpred, span1_count)\n",
    "        #assert span1_count == self.Tpred, f\"Number of span 1 horizons: {span1_count}\\\n",
    "                                            #does not match Tpred: {self.Tpred}\" \n",
    "\n",
    "#         self.spanN_count = len(list(filter(lambda x: x[1] != 1, self.ltsp)))\n",
    "        self.spanN_count = 1\n",
    "        # Setting default components:\n",
    "        if expander is None:\n",
    "            expander = ExpandLayer(self.time_step, self.lead_future, self.future_information)\n",
    "        if hf1 is None:\n",
    "            hf1 = GlobalFutureLayer(self.time_step, self.lead_future, self.future_features_count, out_channels=self.global_hidden_units)\n",
    "        if ht1 is None:\n",
    "            ht1 = HorizonSpecific(self.Tpred, self.time_step, num = self.horizon_specific_hidden_units)\n",
    "        if ht2 is None:\n",
    "            ht2 = HorizonAgnostic(self.horizon_agnostic_hidden_units, self.lead_future)\n",
    "        if h is None:\n",
    "            h = LocalMlp(self.local_mlp_hidden_units, self.local_mlp_output_units)\n",
    "        if span_1 is None:\n",
    "            span_1 = Span1(self.time_step, self.lead_future, self.num_quantiles)\n",
    "        if span_N is None:\n",
    "            span_N = SpanN(self.time_step, self.lead_future, self.num_quantiles, self.spanN_count)\n",
    "\n",
    "        self.expander = expander\n",
    "        self.hf1 = hf1\n",
    "        self.hf2 = hf2\n",
    "        self.ht1 = ht1\n",
    "        self.ht2 = ht2\n",
    "        self.h = h\n",
    "        self.span_1 = span_1\n",
    "        self.span_N = span_N\n",
    "\n",
    "    def forward(self, x, encoded):\n",
    "        xf = x['future_information']\n",
    "        expanded = self.expander(xf)\n",
    "        hf1 = self.hf1(expanded)\n",
    "        hf2 = F.relu(expanded)\n",
    "        \n",
    "        ht = torch.cat((encoded, hf1), dim=-1)\n",
    "        ht1 = self.ht1(ht)\n",
    "        ht2 = self.ht2(ht)\n",
    "        h = torch.cat((ht1, ht2, hf2), dim=-1)\n",
    "        h = self.h(h)\n",
    "        return self.span_1(h)#, self.span_N(h)\n",
    "\n",
    "# submodule\n",
    "\n",
    "class StaticLayer(nn.Module):\n",
    "    def __init__(self, in_channels, time_step, static_features, out_channels = 30, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        #self.static_features = static_features\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.static = nn.Linear(self.in_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x['static_features'][:,:1,:]\n",
    "        x = self.dropout(x)\n",
    "        x = self.static(x)\n",
    "        return x.repeat(1, self.time_step, 1)\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, time_step, timevarying_features, in_channels, out_channels = 30, kernel_size = 2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.time_step = time_step\n",
    "\n",
    "        self.c1 = nn.Conv1d(self.in_channels, self.out_channels, self.kernel_size, dilation = 1)\n",
    "        self.c2 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 2)\n",
    "        self.c3 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size,  dilation = 4)\n",
    "        self.c4 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 8)\n",
    "        self.c5 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 16)\n",
    "        self.c6 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_t = x['timevarying_features'][:, :self.time_step, :]\n",
    "        x_t = x_t.permute(0, 2, 1)\n",
    "        x_t = F.pad(x_t, (1,0), \"constant\", 0)\n",
    "        x_t = self.c1(x_t)\n",
    "        x_t = F.pad(x_t, (2,0), \"constant\", 0)\n",
    "        x_t = self.c2(x_t)\n",
    "        x_t = F.pad(x_t, (4,0), \"constant\", 0)\n",
    "        x_t = self.c3(x_t)\n",
    "        x_t = F.pad(x_t, (8,0), \"constant\", 0)\n",
    "        x_t = self.c4(x_t)\n",
    "        x_t = F.pad(x_t, (16,0), \"constant\", 0)\n",
    "        x_t = self.c5(x_t)\n",
    "        \n",
    "        return x_t.permute(0, 2, 1)\n",
    "\n",
    "class ExpandLayer(nn.Module):\n",
    "    \"\"\"Expands the dimension referred to as `expand_axis` into two\n",
    "    dimensions by applying a sliding window. For example, a tensor of\n",
    "    shape (1, 4, 2) as follows:\n",
    "\n",
    "    [[[0. 1.]\n",
    "      [2. 3.]\n",
    "      [4. 5.]\n",
    "      [6. 7.]]]\n",
    "\n",
    "    where `expand_axis` = 1 and `time_step` = 3 (number of windows) and\n",
    "    `lead_future` = 2 (window length) will become:\n",
    "\n",
    "    [[[[0. 1.]\n",
    "       [2. 3.]]\n",
    "\n",
    "      [[2. 3.]\n",
    "       [4. 5.]]\n",
    "\n",
    "      [[4. 5.]\n",
    "       [6. 7.]]]]\n",
    "\n",
    "    Used for expanding future information tensors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_step : int\n",
    "        Length of the time sequence (number of windows)\n",
    "    lead_future : int\n",
    "        Number of future time points (window length)\n",
    "    expand_axis : int\n",
    "        Axis to expand\"\"\"\n",
    "\n",
    "    def __init__(self, time_step, lead_future, future_information, **kwargs):\n",
    "        super(ExpandLayer, self).__init__(**kwargs)\n",
    "    \n",
    "        self.time_step = time_step\n",
    "        self.future_information = future_information\n",
    "        self.lead_future = lead_future\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First create a matrix of indices, which we will use to slice\n",
    "        # `input` along `expand_axis`. For example, for time_step=3 and\n",
    "        # lead_future=2,\n",
    "        # idx = [[0. 1.]\n",
    "        #        [1. 2.]\n",
    "        #        [2. 3.]]\n",
    "        # We achieve this by doing a broadcast add of\n",
    "        # [[0.] [1.] [2.]] and [[0. 1.]]\n",
    "        idx = torch.add(torch.arange(self.time_step).unsqueeze(axis = 1),\n",
    "                        torch.arange(self.lead_future).unsqueeze(axis = 0))\n",
    "        # Now we slice `input`, taking elements from `input` that correspond to\n",
    "        # the indices in `idx` along the `expand_axis` dimension\n",
    "        return x[:, idx, :]\n",
    "\n",
    "        \n",
    "class GlobalFutureLayer(nn.Module):\n",
    "    def __init__(self, time_step, lead_future, future_features_count, out_channels = 30):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.future_features_count = future_features_count\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.l1 = nn.Linear(self.lead_future * self.future_features_count, out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(-1, self.time_step, self.lead_future * self.future_features_count)\n",
    "        \n",
    "        return self.l1(x)\n",
    "    \n",
    "class HorizonSpecific(nn.Module):\n",
    "    def __init__(self, Tpred, time_step, num = 20):\n",
    "        super().__init__()\n",
    "        self.Tpred = Tpred\n",
    "        self.time_step = time_step\n",
    "        self.num = num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.Tpred * self.num)(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x.view(-1, self.time_step, self.Tpred, 20)\n",
    "\n",
    "class HorizonAgnostic(nn.Module):\n",
    "    def __init__(self, out_channels, lead_future):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.lead_future = lead_future\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.out_channels)(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.unsqueeze(axis = 2)\n",
    "        x = x.repeat(1,1, self.lead_future, 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class LocalMlp(nn.Module):\n",
    "    def __init__(self, hidden, output):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.hidden)(x)\n",
    "        x = F.relu(x)\n",
    "        x = nn.Linear(self.hidden, self.output)(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Span1(nn.Module):\n",
    "    def __init__(self, time_step, lead_future, num_quantiles):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.num_quantiles = num_quantiles\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.num_quantiles)(x)\n",
    "        x = F.relu(x.contiguous().view(-1, x.size(-2), x.size(-1)))\n",
    "        x = x.view(-1, self.time_step, self.lead_future, self.num_quantiles)\n",
    "        x = x.view(-1, self.time_step, self.lead_future*self.num_quantiles)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpanN(nn.Module):\n",
    "    def __init__(self, time_step, lead_future, num_quantiles, spanN_count):\n",
    "        super().__init__()\n",
    "        self.time_step = time_step\n",
    "        self.lead_future = lead_future\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.spanN_count = spanN_count\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.contiguous().view(-1, self.time_step, x.size(-2) * x.size(-1))\n",
    "\n",
    "        x = nn.Linear(x.size(-1), self.spanN_count * self.num_quantiles)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQCNNModel(pl.LightningModule):\n",
    "    def __init__(self, static_features, timevarying_features, future_information, time_step, ltsp, lead_future,\n",
    "                 global_hidden_units, horizon_specific_hidden_units,\n",
    "                 horizon_agnostic_hidden_units, local_mlp_hidden_units, local_mlp_output_units):\n",
    "        super(MQCNNModel, self).__init__()\n",
    "        #self.input_tensor = input_tensor\n",
    "        self.time_step = time_step\n",
    "        self.static_features = static_features\n",
    "        self.num_static_features = len(static_features)\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.num_timevarying_features = len(timevarying_features)\n",
    "        self.future_information = future_information\n",
    "        self.num_future_features = len(future_information)\n",
    "        self.ltsp = ltsp\n",
    "        self.lead_future = lead_future\n",
    "        self.global_hidden_units = global_hidden_units\n",
    "        self.horizon_specific_hidden_units = horizon_specific_hidden_units\n",
    "        self.horizon_agnostic_hidden_units = horizon_agnostic_hidden_units\n",
    "        self.local_mlp_hidden_units = local_mlp_hidden_units\n",
    "        self.local_mlp_output_units = local_mlp_output_units\n",
    "\n",
    "        self.encoder = MQCNNEncoder(self.time_step, self.static_features, self.timevarying_features,\n",
    "                                   self.num_static_features, self.num_timevarying_features)\n",
    "        \n",
    "        self.decoder = MQCNNDecoder(self.time_step, self.lead_future, self.ltsp, self.future_information,\n",
    "                                    self.num_future_features, self.global_hidden_units, self.horizon_specific_hidden_units,\n",
    "                                    self.horizon_agnostic_hidden_units, self.local_mlp_hidden_units,\n",
    "                                    self.local_mlp_output_units)\n",
    "                                    \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        output = self.decoder(x, encoding)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr = 1e-2)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch, batch['targets']\n",
    "                                    \n",
    "        quantiles = torch.tensor([0.5, 0.9]).view(2, 1)\n",
    "\n",
    "        outputs = self(x)\n",
    "\n",
    "        loss = self.loss(outputs, y, quantiles)\n",
    "        pbar = {'train_loss': loss[0] + loss[1]}\n",
    "\n",
    "        return {\"loss\": loss[0] + loss[1], \"progress_bar\": pbar}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "\n",
    "        train_data = Dataset(\"-----enter args here------\")\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size = batch_size)\n",
    "\n",
    "        return train_loader\n",
    "                                    \n",
    "    def loss(self, outputs, targets, quantiles):\n",
    "        l = outputs - targets.repeat_interleave(2, dim=2)\n",
    "        \n",
    "        p50 = torch.mul(torch.where(l > torch.zeros(l.shape), l, torch.zeros_like(l)), 1 - quantiles[0]) + \\\n",
    "            torch.mul(torch.where(l < torch.zeros(l.shape), -l, torch.zeros_like(l)), quantiles[0])\n",
    "    \n",
    "        p90 = torch.mul(torch.where(l > torch.zeros(l.shape), l, torch.zeros_like(l)), 1 - quantiles[1]) + \\\n",
    "            torch.mul(torch.where(l < torch.zeros(l.shape), -l, torch.zeros_like(l)), quantiles[1])\n",
    "        \n",
    "        p50 = p50.mean()\n",
    "        p90 = p90.mean()\n",
    "    \n",
    "        return p50, p90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, static_features, timevarying_features, future_information, \n",
    "                 target, train_time_step, predict_time_step, num_quantiles, ltsp):\n",
    "        \n",
    "        self.data = data\n",
    "        self.train_time_step = train_time_step\n",
    "        self.predict_time_step = predict_time_step\n",
    "        self.num_quantiles = num_quantiles\n",
    "        \n",
    "        self.ltsp_kernel = _ltsp_kernel(predict_time_step, ltsp)\n",
    "        \n",
    "        self.ltsp_idx = _ltsp_idx(time_step = train_time_step, Tpred = predict_time_step)\n",
    "        \n",
    "        \n",
    "        self.static_features = torch.tensor(self.data.\\\n",
    "                              loc[self.data['time_idx'] < self.train_time_step][static_features].\\\n",
    "                to_numpy(np.float64).reshape(-1, self.train_time_step, len(static_features))).float()\n",
    "        \n",
    "        self.timevarying_features = torch.tensor(self.data.\\\n",
    "                              loc[self.data['time_idx'] < self.train_time_step][timevarying_features].\\\n",
    "                to_numpy(np.float64).reshape(-1, self.train_time_step, len(timevarying_features))).float()\n",
    "            \n",
    "        self.future_information = torch.tensor(self.data[future_information].\\\n",
    "                to_numpy(np.float64).reshape(-1, (self.train_time_step + self.predict_time_step), len(future_information))).float()\n",
    "        \n",
    "        self.targets = torch.tensor(self.data[target].\\\n",
    "            to_numpy(np.float64).reshape(-1, (self.train_time_step + self.predict_time_step))).float()\n",
    "        \n",
    "        self.targets = _apply_ltsp_kernel(self.targets, self.ltsp_idx, self.ltsp_kernel)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.timevarying_features.shape[1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        static_features = self.static_features[idx, :, :]\n",
    "        timevarying_features = self.timevarying_features[idx, :, :]\n",
    "        future_information = self.future_information[idx, :, :]\n",
    "        targets = self.targets[idx, :, :]\n",
    "        \n",
    "        return dict(static_features = static_features, timevarying_features = timevarying_features,\n",
    "                    future_information = future_information, targets = targets)\n",
    "    \n",
    "def _ltsp_idx(time_step, Tpred):\n",
    "        idx = np.arange(time_step).reshape(-1, 1) + np.arange(Tpred)\n",
    "        return torch.tensor(idx)\n",
    "\n",
    "def _ltsp_kernel(Tpred, ltsp, normalize = True):\n",
    "    \n",
    "        ltsp_count = len(ltsp)\n",
    "        kernel = np.zeros((Tpred, ltsp_count), dtype = 'float32')\n",
    "        for i in range(len(ltsp)):\n",
    "            lead_time = ltsp[i][0]\n",
    "            span = ltsp[i][1]\n",
    "            if normalize:\n",
    "                kernel[lead_time:lead_time + span, i] = 1.0/span\n",
    "            else:\n",
    "                kernel[lead_time:lead_time + span, i] = 1.0\n",
    "\n",
    "        return torch.tensor(kernel)\n",
    "\n",
    "def _apply_ltsp_kernel(s, ltsp_idx, ltsp_kernel):\n",
    "        s_ltsp = s[:, ltsp_idx].float()\n",
    "        \n",
    "        return s_ltsp @ ltsp_kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_stallion_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>sku</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>...</th>\n",
       "      <th>independence_day</th>\n",
       "      <th>revolution_day_memorial</th>\n",
       "      <th>regional_games</th>\n",
       "      <th>fifa_u_17_world_cup</th>\n",
       "      <th>football_gold_cup</th>\n",
       "      <th>beer_capital</th>\n",
       "      <th>music_fest</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Agency_25</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>25.845238</td>\n",
       "      <td>1264.162234</td>\n",
       "      <td>1152.473405</td>\n",
       "      <td>111.688829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.835008</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Agency_29</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>8.7480</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>498567142</td>\n",
       "      <td>762225057</td>\n",
       "      <td>27.584615</td>\n",
       "      <td>1316.098485</td>\n",
       "      <td>1296.804924</td>\n",
       "      <td>19.293561</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.465966</td>\n",
       "      <td>177</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19532</th>\n",
       "      <td>Agency_47</td>\n",
       "      <td>SKU_01</td>\n",
       "      <td>4.9680</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>454252482</td>\n",
       "      <td>789624076</td>\n",
       "      <td>30.665957</td>\n",
       "      <td>1269.250000</td>\n",
       "      <td>1266.490490</td>\n",
       "      <td>2.759510</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217413</td>\n",
       "      <td>322</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Agency_53</td>\n",
       "      <td>SKU_07</td>\n",
       "      <td>21.6825</td>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>480693900</td>\n",
       "      <td>791658684</td>\n",
       "      <td>29.197727</td>\n",
       "      <td>1193.842373</td>\n",
       "      <td>1128.124395</td>\n",
       "      <td>65.717978</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.504745</td>\n",
       "      <td>240</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>Agency_17</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>960.5520</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>515468092</td>\n",
       "      <td>871204688</td>\n",
       "      <td>23.608120</td>\n",
       "      <td>1338.334248</td>\n",
       "      <td>1232.128069</td>\n",
       "      <td>106.206179</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.935699</td>\n",
       "      <td>259</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>Agency_05</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>1184.6535</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>425528909</td>\n",
       "      <td>734443953</td>\n",
       "      <td>28.668254</td>\n",
       "      <td>1369.556376</td>\n",
       "      <td>1161.135214</td>\n",
       "      <td>208.421162</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.218151</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19204</th>\n",
       "      <td>Agency_11</td>\n",
       "      <td>SKU_05</td>\n",
       "      <td>5.5593</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>623319783</td>\n",
       "      <td>1049868815</td>\n",
       "      <td>31.915385</td>\n",
       "      <td>1922.486644</td>\n",
       "      <td>1651.307674</td>\n",
       "      <td>271.178970</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.105636</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>Agency_48</td>\n",
       "      <td>SKU_04</td>\n",
       "      <td>4275.1605</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>509281531</td>\n",
       "      <td>892192092</td>\n",
       "      <td>26.767857</td>\n",
       "      <td>1761.258209</td>\n",
       "      <td>1546.059670</td>\n",
       "      <td>215.198539</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.218455</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>Agency_07</td>\n",
       "      <td>SKU_21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>544203593</td>\n",
       "      <td>761469815</td>\n",
       "      <td>28.987755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>Agency_21</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>46.3608</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>589969396</td>\n",
       "      <td>940912941</td>\n",
       "      <td>32.478910</td>\n",
       "      <td>1675.922116</td>\n",
       "      <td>1413.571789</td>\n",
       "      <td>262.350327</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.654088</td>\n",
       "      <td>181</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          agency     sku     volume       date  industry_volume  soda_volume  \\\n",
       "291    Agency_25  SKU_03     0.5076 2013-01-01        492612703    718394219   \n",
       "871    Agency_29  SKU_02     8.7480 2015-01-01        498567142    762225057   \n",
       "19532  Agency_47  SKU_01     4.9680 2013-09-01        454252482    789624076   \n",
       "2089   Agency_53  SKU_07    21.6825 2013-10-01        480693900    791658684   \n",
       "9755   Agency_17  SKU_02   960.5520 2015-03-01        515468092    871204688   \n",
       "7561   Agency_05  SKU_03  1184.6535 2014-02-01        425528909    734443953   \n",
       "19204  Agency_11  SKU_05     5.5593 2017-08-01        623319783   1049868815   \n",
       "8781   Agency_48  SKU_04  4275.1605 2013-03-01        509281531    892192092   \n",
       "2540   Agency_07  SKU_21     0.0000 2015-10-01        544203593    761469815   \n",
       "12084  Agency_21  SKU_03    46.3608 2017-04-01        589969396    940912941   \n",
       "\n",
       "       avg_max_temp  price_regular  price_actual    discount  ...  \\\n",
       "291       25.845238    1264.162234   1152.473405  111.688829  ...   \n",
       "871       27.584615    1316.098485   1296.804924   19.293561  ...   \n",
       "19532     30.665957    1269.250000   1266.490490    2.759510  ...   \n",
       "2089      29.197727    1193.842373   1128.124395   65.717978  ...   \n",
       "9755      23.608120    1338.334248   1232.128069  106.206179  ...   \n",
       "7561      28.668254    1369.556376   1161.135214  208.421162  ...   \n",
       "19204     31.915385    1922.486644   1651.307674  271.178970  ...   \n",
       "8781      26.767857    1761.258209   1546.059670  215.198539  ...   \n",
       "2540      28.987755       0.000000      0.000000    0.000000  ...   \n",
       "12084     32.478910    1675.922116   1413.571789  262.350327  ...   \n",
       "\n",
       "       independence_day  revolution_day_memorial  regional_games  \\\n",
       "291                   0                        0               0   \n",
       "871                   0                        0               0   \n",
       "19532                 1                        0               0   \n",
       "2089                  0                        0               0   \n",
       "9755                  0                        0               0   \n",
       "7561                  0                        0               0   \n",
       "19204                 0                        0               0   \n",
       "8781                  0                        0               0   \n",
       "2540                  0                        0               0   \n",
       "12084                 0                        0               0   \n",
       "\n",
       "       fifa_u_17_world_cup  football_gold_cup  beer_capital  music_fest  \\\n",
       "291                      0                  0             0           0   \n",
       "871                      0                  0             0           0   \n",
       "19532                    0                  0             0           0   \n",
       "2089                     0                  0             1           0   \n",
       "9755                     0                  0             0           1   \n",
       "7561                     0                  0             0           0   \n",
       "19204                    0                  0             0           0   \n",
       "8781                     0                  0             0           1   \n",
       "2540                     0                  0             0           0   \n",
       "12084                    0                  0             0           0   \n",
       "\n",
       "       discount_in_percent  timeseries  time_idx  \n",
       "291               8.835008         228         0  \n",
       "871               1.465966         177        24  \n",
       "19532             0.217413         322         8  \n",
       "2089              5.504745         240         9  \n",
       "9755              7.935699         259        26  \n",
       "7561             15.218151          21        13  \n",
       "19204            14.105636          17        55  \n",
       "8781             12.218455         151         2  \n",
       "2540              0.000000         300        33  \n",
       "12084            15.654088         181        51  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# add additional features\n",
    "\n",
    "# show sample data\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "data_sorted = data.sort_values(['agency', 'sku', 'date'])\n",
    "\n",
    "data_sorted = pd.get_dummies(data_sorted, columns=['month'])\n",
    "\n",
    "static_cols=['avg_population_2017']\n",
    "timevarying_cols=['volume', 'industry_volume', 'soda_volume', 'price_regular']\n",
    "future_cols=['month_1', 'month_2','month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8',\n",
    "                    'month_9', 'month_10', 'month_11', 'month_12', 'price_regular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltsp = [(i, 1) for i in range(6)]\n",
    "len(ltsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = Dataset(data_sorted,\n",
    "                   static_features = static_cols,\n",
    "                   timevarying_features = timevarying_cols,\n",
    "                   future_information = future_cols,\n",
    "                   target=['volume'], \n",
    "                   train_time_step=54, \n",
    "                   predict_time_step=6,\n",
    "                   num_quantiles = 2,\n",
    "                   ltsp = ltsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MQCNN = MQCNNModel(static_cols, timevarying_cols, future_cols, \n",
    "                   54, ltsp, 6, 50, 20, 100, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | encoder | MQCNNEncoder | 9 K   \n",
      "1 | decoder | MQCNNDecoder | 3 K   \n",
      "/Users/abkatoch/opt/anaconda3/envs/forecastingenv/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55509c967c55464da54b57cbad77d0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abkatoch/opt/anaconda3/envs/forecastingenv/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(MQCNN, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dev_debugger',\n",
       " 'config_validator',\n",
       " 'data_connector',\n",
       " 'optimizer_connector',\n",
       " 'accelerator_connector',\n",
       " 'logger_connector',\n",
       " 'model_connector',\n",
       " 'precision_connector',\n",
       " 'callback_connector',\n",
       " 'debugging_connector',\n",
       " 'training_tricks_connector',\n",
       " 'profile_connector',\n",
       " 'checkpoint_connector',\n",
       " 'slurm_connector',\n",
       " 'tuner',\n",
       " 'accelerator_backend',\n",
       " 'evaluation_loop',\n",
       " 'train_loop',\n",
       " 'plugin_connector',\n",
       " 'weights_summary',\n",
       " 'model',\n",
       " 'shown_warnings',\n",
       " 'resume_from_checkpoint',\n",
       " '_default_root_dir',\n",
       " '_weights_save_path',\n",
       " 'callbacks',\n",
       " '_progress_bar_callback',\n",
       " 'lr_schedulers',\n",
       " 'optimizers',\n",
       " 'optimizer_frequencies',\n",
       " 'datamodule',\n",
       " 'prepare_data_per_node',\n",
       " 'check_val_every_n_epoch',\n",
       " 'reload_dataloaders_every_epoch',\n",
       " '_is_data_prepared',\n",
       " 'terminate_on_nan',\n",
       " 'gradient_clip_val',\n",
       " 'track_grad_norm',\n",
       " 'accumulate_grad_batches',\n",
       " 'accumulation_scheduler',\n",
       " 'truncated_bptt_steps',\n",
       " 'deterministic',\n",
       " 'distributed_backend',\n",
       " 'benchmark',\n",
       " 'num_nodes',\n",
       " 'log_gpu_memory',\n",
       " 'sync_batchnorm',\n",
       " 'tpu_cores',\n",
       " 'on_tpu',\n",
       " 'tpu_id',\n",
       " 'num_processes',\n",
       " 'gpus',\n",
       " 'data_parallel_device_ids',\n",
       " 'root_gpu',\n",
       " 'root_device',\n",
       " 'on_gpu',\n",
       " 'use_tpu',\n",
       " 'tpu_local_core_rank',\n",
       " 'tpu_global_core_rank',\n",
       " 'use_dp',\n",
       " 'use_ddp',\n",
       " 'use_ddp2',\n",
       " 'use_horovod',\n",
       " 'use_single_gpu',\n",
       " 'world_size',\n",
       " 'interactive_ddp_procs',\n",
       " 'is_slurm_managing_tasks',\n",
       " 'node_rank',\n",
       " 'local_rank',\n",
       " 'global_rank',\n",
       " 'on_colab_kaggle',\n",
       " 'replace_sampler_ddp',\n",
       " 'global_step',\n",
       " 'current_epoch',\n",
       " 'interrupted',\n",
       " 'should_stop',\n",
       " '_state',\n",
       " 'total_batch_idx',\n",
       " 'batch_idx',\n",
       " 'num_training_batches',\n",
       " 'train_dataloader',\n",
       " 'max_epochs',\n",
       " 'min_epochs',\n",
       " 'max_steps',\n",
       " 'min_steps',\n",
       " 'num_sanity_val_steps',\n",
       " 'num_val_batches',\n",
       " 'num_sanity_val_batches',\n",
       " 'num_test_batches',\n",
       " 'test_dataloaders',\n",
       " 'val_dataloaders',\n",
       " 'running_sanity_check',\n",
       " 'testing',\n",
       " 'tested_ckpt_path',\n",
       " 'verbose_test',\n",
       " 'auto_lr_find',\n",
       " 'auto_scale_batch_size',\n",
       " 'profiler',\n",
       " 'logger',\n",
       " 'flush_logs_every_n_steps',\n",
       " 'log_every_n_steps',\n",
       " 'split_idx',\n",
       " 'fast_dev_run',\n",
       " 'limit_train_batches',\n",
       " 'limit_val_batches',\n",
       " 'limit_test_batches',\n",
       " 'val_check_interval',\n",
       " 'overfit_batches',\n",
       " 'autocast_original_forward',\n",
       " 'precision',\n",
       " 'scaler',\n",
       " 'amp_level',\n",
       " 'amp_backend',\n",
       " 'val_check_batch',\n",
       " 'hiddens',\n",
       " '__module__',\n",
       " '__init__',\n",
       " 'fit',\n",
       " 'train',\n",
       " 'run_evaluation',\n",
       " 'run_test',\n",
       " 'run_sanity_check',\n",
       " 'test',\n",
       " '_Trainer__test_using_best_weights',\n",
       " '_Trainer__test_given_model',\n",
       " 'tune',\n",
       " 'call_setup_hook',\n",
       " 'call_hook',\n",
       " '__doc__',\n",
       " '__abstractmethods__',\n",
       " '_abc_registry',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '__annotations__',\n",
       " 'use_amp',\n",
       " 'callback_metrics',\n",
       " 'logged_metrics',\n",
       " 'progress_bar_metrics',\n",
       " 'state',\n",
       " 'is_global_zero',\n",
       " 'slurm_job_id',\n",
       " 'default_attributes',\n",
       " 'get_deprecated_arg_names',\n",
       " 'from_argparse_args',\n",
       " 'parse_argparser',\n",
       " 'match_env_arguments',\n",
       " 'add_argparse_args',\n",
       " 'num_gpus',\n",
       " 'data_parallel',\n",
       " 'progress_bar_callback',\n",
       " 'progress_bar_dict',\n",
       " 'disable_validation',\n",
       " 'enable_validation',\n",
       " 'default_root_dir',\n",
       " 'weights_save_path',\n",
       " 'checkpoint_callback',\n",
       " 'checkpoint_callbacks',\n",
       " 'save_checkpoint',\n",
       " 'get_model',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__',\n",
       " 'setup',\n",
       " 'teardown',\n",
       " 'on_init_start',\n",
       " 'on_init_end',\n",
       " 'on_fit_start',\n",
       " 'on_fit_end',\n",
       " 'on_sanity_check_start',\n",
       " 'on_sanity_check_end',\n",
       " 'on_train_epoch_start',\n",
       " 'on_train_epoch_end',\n",
       " 'on_validation_epoch_start',\n",
       " 'on_validation_epoch_end',\n",
       " 'on_test_epoch_start',\n",
       " 'on_test_epoch_end',\n",
       " 'on_epoch_start',\n",
       " 'on_epoch_end',\n",
       " 'on_train_start',\n",
       " 'on_train_end',\n",
       " 'on_pretrain_routine_start',\n",
       " 'on_pretrain_routine_end',\n",
       " 'on_batch_start',\n",
       " 'on_batch_end',\n",
       " 'on_train_batch_start',\n",
       " 'on_train_batch_end',\n",
       " 'on_validation_batch_start',\n",
       " 'on_validation_batch_end',\n",
       " 'on_test_batch_start',\n",
       " 'on_test_batch_end',\n",
       " 'on_validation_start',\n",
       " 'on_validation_end',\n",
       " 'on_test_start',\n",
       " 'on_test_end',\n",
       " 'on_keyboard_interrupt',\n",
       " 'on_save_checkpoint',\n",
       " 'on_load_checkpoint',\n",
       " 'is_function_implemented',\n",
       " 'has_arg',\n",
       " 'init_optimizers',\n",
       " 'configure_schedulers',\n",
       " 'reinit_scheduler_properties',\n",
       " 'metrics_to_scalars',\n",
       " 'process_dict_result',\n",
       " 'reduce_distributed_output',\n",
       " 'print_nan_gradients',\n",
       " 'detect_nan_tensors',\n",
       " '_worker_check',\n",
       " 'auto_add_sampler',\n",
       " 'replace_sampler',\n",
       " '_get_distributed_sampler',\n",
       " 'reset_train_dataloader',\n",
       " '_reset_eval_dataloader',\n",
       " 'reset_val_dataloader',\n",
       " 'reset_test_dataloader',\n",
       " 'request_dataloader',\n",
       " '_flatten_dl_only']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
