{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticLayer(nn.Module):\n",
    "    def __init__(self,in_channels, Trnn, static_features, out_channels = 30, dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.Trnn = Trnn\n",
    "        self.static_features = static_features\n",
    "        self.dropout = dropout\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.static = nn.Linear(self.in_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[self.static_features].squeeze(1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.static(x)\n",
    "        return x.unsqueeze(1).repeat(1, self.Trnn, 1)\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, timevarying_features, out_channels = 30, kernel_size = 2):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.timevarying_features = timevarying_features\n",
    "\n",
    "        c1 = nn.Conv1d(self.in_channels, self.out_channels, self.kernel_size, dilation = 1)\n",
    "        c2 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 2)\n",
    "        c3 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 4)\n",
    "        c4 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 8)\n",
    "        c5 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 16)\n",
    "        c6 = nn.Conv1d(self.out_channels, self.out_channels, self.kernel_size, dilation = 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_t = x[self.timevarying_features]\n",
    "        x_t = x_t.permute(0, 2, 1)\n",
    "        x_t = F.pad(x_t, (0,0), \"constant\", 0)\n",
    "        x_t = c1(x_t)\n",
    "        x_t = F.pad(x_t, (2,0), \"constant\", 0)\n",
    "        x_t = c2(x_t)\n",
    "        x_t = F.pad(x_t, (4,0), \"constant\", 0)\n",
    "        x_t = c3(x_t)\n",
    "        x_t = F.pad(x_t, (8,0), \"constant\", 0)\n",
    "        x_t = c4(x_t)\n",
    "        x_t = F.pad(x_t, (16,0), \"constant\", 0)\n",
    "        x_t = c5(x_t)\n",
    "        x_t = F.pad(x_t, (32,0), \"constant\", 0)\n",
    "        x_t = c6(x_t)\n",
    "        \n",
    "        return x_t.permute(0, 2, 1)\n",
    "\n",
    "class ExpandLayer(nn.Module):\n",
    "    \"\"\"Expands the dimension referred to as `expand_axis` into two\n",
    "    dimensions by applying a sliding window. For example, a tensor of\n",
    "    shape (1, 4, 2) as follows:\n",
    "\n",
    "    [[[0. 1.]\n",
    "      [2. 3.]\n",
    "      [4. 5.]\n",
    "      [6. 7.]]]\n",
    "\n",
    "    where `expand_axis` = 1 and `Trnn` = 3 (number of windows) and\n",
    "    `lead_future` = 2 (window length) will become:\n",
    "\n",
    "    [[[[0. 1.]\n",
    "       [2. 3.]]\n",
    "\n",
    "      [[2. 3.]\n",
    "       [4. 5.]]\n",
    "\n",
    "      [[4. 5.]\n",
    "       [6. 7.]]]]\n",
    "\n",
    "    Used for expanding future information tensors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Trnn : int\n",
    "        Length of the time sequence (number of windows)\n",
    "    lead_future : int\n",
    "        Number of future time points (window length)\n",
    "    expand_axis : int\n",
    "        Axis to expand\"\"\"\n",
    "\n",
    "    def __init__(self, Trnn, lead_future, future_information, **kwargs):\n",
    "        super(ExpandLayer, self).__init__(**kwargs)\n",
    "    \n",
    "        self.Trnn = Trnn\n",
    "        self.future_information = future_information\n",
    "        self.lead_future = lead_future\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First create a matrix of indices, which we will use to slice\n",
    "        # `input` along `expand_axis`. For example, for Trnn=3 and\n",
    "        # lead_future=2,\n",
    "        # idx = [[0. 1.]\n",
    "        #        [1. 2.]\n",
    "        #        [2. 3.]]\n",
    "        # We achieve this by doing a broadcast add of\n",
    "        # [[0.] [1.] [2.]] and [[0. 1.]]\n",
    "        x = x[self.future_information]\n",
    "        idx = torch.add(torch.arange(self.Trnn).unsqueeze(axis = 1), \n",
    "                        torch.arange(self.lead_future).unsqueeze(axis = 0))\n",
    "        # Now we slice `input`, taking elements from `input` that correspond to\n",
    "        # the indices in `idx` along the `expand_axis` dimension\n",
    "        return x[:, idx, :]\n",
    "\n",
    "\n",
    "    \n",
    "class HorizonSpecific(nn.Module):\n",
    "    def __init__(self, Tpred, Trnn, num = 20):\n",
    "        super().__init__()\n",
    "        self.Tpred = Tpred\n",
    "        self.Trnn = Trnn\n",
    "        self.num = num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = nn.Linear(x.size(-1), self.Tpred*self.num)(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x.view(-1, self.Trnn, self.Tpred, 20)\n",
    "\n",
    "class HorizonAgnostic(nn.Module):\n",
    "    def __init__(self, out_channels, lead_future):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.lead_future = lead_future\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.out_channels)(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.unsqueeze(axis = 2)\n",
    "        x = x.repeat(1,1, self.lead_future, 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class LocalMlp(nn.Module):\n",
    "    def __init__(self, hidden, output):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.output = output\n",
    "        self.l2 = nn.Linear(self.hidden, self.output)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = nn.Linear(x.size(-1), self.hidden)(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Span1(nn.Module):\n",
    "    def __init__(self, Trnn, lead_future, num_quantiles):\n",
    "        super().__init__()\n",
    "        self.Trnn = Trnn\n",
    "        self.lead_future = lead_future\n",
    "        self.num_quantiles = num_quantiles\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.Linear(x.size(-1), self.num_quantiles)\n",
    "        x = F.relu(x.contiguous().view(-1, x.size(-2), x.size(-1)))\n",
    "        x = x.view(-1, self.Trnn, self.lead_future, self.num_quantiles)\n",
    "        x = x.view(-1, self.Trnn, self.lead_future*self.num_quantiles)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SpanN(nn.Module):\n",
    "    def __init__(self, Trnn, lead_future, num_quantiles, spanN_count):\n",
    "        super().__init__()\n",
    "        self.Trnn = Trnn\n",
    "        self.lead_future = lead_future\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.spanN_count = spanN_count\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        x = x.contiguous().view(-1, self.Trnn, x.size(-2) * x.size(-1))\n",
    "\n",
    "        x = nn.Linear(x.size(-1), self.spanN_count * self.num_quantiles)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalFutureLayer(nn.Module):\n",
    "    def __init__(self, lead_future, future_features_count, out_channels = 30):\n",
    "        super(GlobalFutureLayer, self).__init__()\n",
    "        self.lead_future = lead_future\n",
    "        self.future_features_count = future_features_count\n",
    "        self.out_channels = out_channels\n",
    "        self.l1 = nn.Linear(self.lead_future*self.future_features_count, self.out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.Trnn, self.lead_future * self.future_features_count)\n",
    "        \n",
    "        return self.l1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MQCNNModel(nn.Module):\n",
    "    def __init__(self, Trnn, static_features, timevarying_features, future_information, ltsp, lead_future):\n",
    "        super(MQCNNModel, self).__init__()\n",
    "        \n",
    "        self.Trnn = Trnn\n",
    "        self.static_features = static_features\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.future_information = future_information\n",
    "        self.ltsp = ltsp\n",
    "        self.lead_future = lead_future\n",
    "\n",
    "        self.encoder = MQCNNEncoder(self.Trnn, self.static_features, self.timevarying_features)\n",
    "        self.decoder = MQCNNDecoder(self.Trnn, self.lead_future, self.ltsp, self.future_information, self.future_information)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        x = self.decoder(encoding, x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MQCNNEncoder(nn.Module):\n",
    "    def __init__(self, Trnn, static_features, timevarying_features):\n",
    "        super(MQCNNEncoder, self).__init__()\n",
    "        self.Trnn = Trnn\n",
    "        self.static_features = static_features\n",
    "        self.timevarying_features = timevarying_features\n",
    "        self.static = StaticLayer(in_channels = len(self.static_features),\n",
    "                                  Trnn = self.Trnn,\n",
    "                                  static_features = self.static_features)\n",
    "\n",
    "        self.conv = ConvLayer(in_channels = len(self.timevarying_features),\n",
    "                             timevarying_features = self.timevarying_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_s = self.static(x)\n",
    "        x_t = self.conv(x)\n",
    "\n",
    "        return torch.cat((x_s, x_t), axis = 1)\n",
    "\n",
    "\n",
    "class MQCNNDecoder(nn.Module):\n",
    "    \"\"\"Decoder implementation for MQCNN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config\n",
    "        Configurations\n",
    "    ltsp : list of tuple of int\n",
    "        List of lead-time / span tuples to make predictions for\n",
    "    expander : HybridBlock\n",
    "        Overrides default future data expander if not None\n",
    "    hf1 : HybridBlock\n",
    "        Overrides default global future layer if not None\n",
    "    hf2 : HybridBlock\n",
    "        Overrides default local future layer if not None\n",
    "    ht1 : HybridBlock\n",
    "        Overrides horizon-specific layer if not None\n",
    "    ht2 : HybridBlock\n",
    "        Overrides horizon-agnostic layer if not None\n",
    "    h : HybridBlock\n",
    "        Overrides local MLP if not None\n",
    "    span_1 : HybridBlock\n",
    "        Overrides span 1 layer if not None\n",
    "    span_N : HybridBlock\n",
    "        Overrides span N layer if not None\n",
    "\n",
    "    Inputs:\n",
    "        - **xf** : Future data of shape\n",
    "            (batch_size, Trnn + lead_future - 1, num_future_ts_features)\n",
    "        - **encoded** : Encoded input tensor of shape\n",
    "            (batch_size, Trnn, n) for some n\n",
    "    Outputs:\n",
    "        - **pred_1** :  Span 1 predictions of shape\n",
    "            (batch_size, Trnn, Tpred * num_quantiles)\n",
    "        - **pred_N** : Span N predictions of shape\n",
    "            (batch_size, Trnn, span_N_count * num_quantiles)\n",
    "\n",
    "        In both outputs, the last dimensions has the predictions grouped\n",
    "        together by quantile. For example, the quantiles are P10 and P90\n",
    "        then the span 1 predictions will be:\n",
    "        Tpred_0_p50, Tpred_1_p50, ..., Tpred_N_p50, Tpred_0_p90,\n",
    "        Tpred_1_p90, ... Tpred_N_90\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Trnn, lead_future, future_information, ltsp, num_quantiles = 2):\n",
    "        super(MQCNNDecoder, self).__init__()\n",
    "        self.future_features_count = len(future_information)\n",
    "        self.future_information = future_information\n",
    "        self.Trnn = Trnn\n",
    "        self.lead_future = lead_future\n",
    "        self.ltsp = ltsp\n",
    "        self.num_quantiles = num_quantiles\n",
    "\n",
    "        # We assume that Tpred == span1_count.\n",
    "        self.Tpred = max(map(lambda x: x[0] + x[1], self.ltsp))\n",
    "        span1_count = len(list(filter(lambda x: x[1] == 1, self.ltsp)))\n",
    "        self.spanN_count = len(list(filter(lambda x: x[1] != 1, self.ltsp)))\n",
    "\n",
    "        self.expander = ExpandLayer(self.Trnn, self.lead_future, self.future_information)\n",
    "        self.hf1 = GlobalFutureLayer(self.lead_future, self.future_features_count, 30)\n",
    "        self.ht1 = HorizonSpecific(self.Tpred, self.Trnn, 20)\n",
    "        self.ht2 = HorizonAgnostic(100, self.lead_future)\n",
    "        self.h = LocalMlp(50, 10)\n",
    "        self.span_1 = Span1(self.Trnn, self.lead_future, self.num_quantiles)\n",
    "        self.span_N = SpanN(self.Trnn, self.lead_future, self.num_quantiles, self.spanN_count)\n",
    "\n",
    "    def forward(self, F, x, encoded):\n",
    "        xf = x[self.future_information]\n",
    "        expanded = self.expander(xf)\n",
    "        hf1 = self.hf1(expanded)\n",
    "        hf2 = F.tanh(expanded)\n",
    "\n",
    "        ht = torch.cat(encoded, hf1, dim=-1)\n",
    "        ht1 = self.ht1(ht)\n",
    "        ht2 = self.ht2(ht)\n",
    "        h = torch.cat(ht1, ht2, hf2, dim=-1)\n",
    "        h = self.h(h)\n",
    "        return self.span_1(h), self.span_N(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-6b92767c5623>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m456\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m25\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcols\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'cols' is not defined"
     ]
    }
   ],
   "source": [
    "x = pd.DataFrame(torch.rand(1000, 456, 25))\n",
    "x.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[tensor(0.0743), tensor(0.5964), tensor(0.664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[tensor(0.2655), tensor(0.3815), tensor(0.782...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[tensor(0.1067), tensor(0.8012), tensor(0.605...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[tensor(0.4733), tensor(0.5251), tensor(0.411...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[tensor(0.5630), tensor(0.3831), tensor(0.652...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [[tensor(0.0743), tensor(0.5964), tensor(0.664...\n",
       "1  [[tensor(0.2655), tensor(0.3815), tensor(0.782...\n",
       "2  [[tensor(0.1067), tensor(0.8012), tensor(0.605...\n",
       "3  [[tensor(0.4733), tensor(0.5251), tensor(0.411...\n",
       "4  [[tensor(0.5630), tensor(0.3831), tensor(0.652..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i in range(25):\n",
    "    cols.append(f'col{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_features = ['col0', 'col1', 'col2', 'col3', 'col4']\n",
    "\n",
    "timevarying_features = ['col5','col6','col7','col8','col9','col10','col11','col12','col13','col14','col15','col16','col17','col18','col19', 'col20', 'col21', 'col22', 'col23', 'col24', 'col25']\n",
    "\n",
    "future_information = ['col19', 'col20', 'col21', 'col22', 'col23', 'col24', 'col25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ltsp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-433e7cfe4608>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mltsp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'ltsp' is not defined"
     ]
    }
   ],
   "source": [
    "max(map(lambda x: x[0] + x[1], ltsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(filter(lambda x: x[1] == 1, ltsp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MQCNNModel(Trnn = 365, \n",
    "               lead_future = 91, \n",
    "               static_features= static_features, \n",
    "               timevarying_features = timevarying_features, \n",
    "               future_information = future_information, \n",
    "               ltsp = ltsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_f = torch.rand(500, 456, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 365, 30])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(500, 456, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = x[:, :365, 10:40]\n",
    "x_f = x[:, :, 40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = torch.rand(500, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 10]), torch.Size([500, 365, 30]), torch.Size([500, 456, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_s.shape, x_t.shape, x_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nn.Linear(10, 30)(x_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = F.pad(x_t.permute(0, 2, 1), (1, 0), \"constant\", 0)\n",
    "c1 = nn.Conv1d(30, 30, 2, dilation = 1)(c1)\n",
    "c2 = F.pad(c1, (2, 0), \"constant\", 0)\n",
    "c2 = nn.Conv1d(30, 30, 2, dilation = 2)(c2)\n",
    "c3 = F.pad(c2, (4, 0), \"constant\", 0)\n",
    "c3 = nn.Conv1d(30, 30, 2,  dilation = 4)(c3)\n",
    "c4 = F.pad(c3, (8, 0), \"constant\", 0)\n",
    "c4 = nn.Conv1d(30, 30, 2, dilation = 8)(c4)\n",
    "c5 = F.pad(c4, (16, 0), \"constant\", 0)\n",
    "c5 = nn.Conv1d(30, 30, 2, dilation = 16)(c5)\n",
    "c6 = F.pad(c5, (32, 0), \"constant\", 0)\n",
    "c6 = nn.Conv1d(30, 30, 2, dilation = 32)(c6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = c6.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.unsqueeze(1).repeat(1, 365, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 365, 30])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = torch.cat((s, t), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 365, 60])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = torch.cat((hf1, encoding), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 365, 90])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 365, 1820])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(90, 1820)(ht).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht1 = nn.Linear(90, 1820)(ht).view(-1, 365, 91, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht2 = nn.Linear(90, 100)(ht).unsqueeze(axis = 2).repeat(1, 1, 91, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2 = F.relu(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 365, 91, 10]),\n",
       " torch.Size([500, 365, 91, 20]),\n",
       " torch.Size([500, 365, 91, 100]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded.shape, ht1.shape, ht2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.cat((ht1, ht2, hf2), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class local_mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(130, 50)\n",
    "        self.l2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = local_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = l(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = nn.Linear(10, 2)(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = F.leaky_relu(h2.contiguous().view(-1, h3.size(-2), h3.size(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.view(-1, 365, 91, 2).view(500, 365, 182).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = h2.permute(0, 1, 3, 2).contiguous().view(-1, 365, h2.size(-2) * h2.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear(h4.size(-1), 40)(h4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.DataLoader):\n",
    "    \n",
    "    def __init__(self, data, static_features, time_varying_known, time_varying_unknown, targets):\n",
    "        \n",
    "        self.targets = targets\n",
    "        self.data = data\n",
    "        self.static = static_features\n",
    "        self.tv_known = time_varying_known\n",
    "        self.tv_unknown = time_varying_unknown\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.features['time_idx'].unique())\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        print(self.features)\n",
    "        \n",
    "        X_static = self.data.loc[self.features['time_idx'] == index, [self.static]]\n",
    "        X_tv_known = self.data.loc[self.features['time_idx'] == index, [self.tv_known]]\n",
    "        X_tv_unknown = self.data.loc[self.features['time_idx'] == index, [self.tv_unknown]]\n",
    "        Y = self.targets.loc[self.targets['time_idx'] == index]\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(500, 456, 50)\n",
    "y = x[:, :, 50]\n",
    "static = x[:, 0, :10]\n",
    "tv_known = x[:, 10:20]\n",
    "tv_unknown = [20:50]\n",
    "d = Dataset(x, static, tv_known, tv_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(500, 1, 10).squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}